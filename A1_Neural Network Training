{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM8J+4NP4z1dgSMMnFPMEll",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hiu04/NLP-Deep-Learning/blob/main/Assignment1_DeepLearning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MMAI 5500 â€“ Assignment 1\n",
        "\n",
        "Your task is to code and train a neural network in Python using only NumPy.\n",
        "That is, the only library that you are allowed to import is NumPy. It should\n",
        "be imported with import numpy as np. Luckily some parts of the code are\n",
        "provided (see the Code section below).\n",
        "\n",
        "**The network architecture**\n",
        "\n",
        "The network should have three fully connected weight layers, *three inputs and\n",
        "three-class softmax output*. The first hidden layer should have four neurons and\n",
        "the second eight neurons. All neurons should have a single bias. See the diagram\n",
        "below for a visual description.\n",
        "\n",
        "**Data**\n",
        "\n",
        "The networks should be trained and validated on the data provided in\n",
        "assing1_data.csv."
      ],
      "metadata": {
        "id": "SUxSLAOlikVg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "Gz5LuQg7ijS7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data using NumPy\n",
        "fname = 'assign1_data.csv'\n",
        "data = np.genfromtxt(fname, dtype='float', delimiter=',', skip_header=1)\n",
        "X, y = data[:, :-1], data[:, -1].astype(int) # change float to integer\n",
        "X_train, y_train = X[:400], y[:400]\n",
        "X_test, y_test = X[400:], y[400:]"
      ],
      "metadata": {
        "id": "BYq4-1Zdjxc9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data"
      ],
      "metadata": {
        "id": "3PYzLVzhkAyG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Layer\n",
        "- A dense (aka fully connected) layer"
      ],
      "metadata": {
        "id": "Pzo7vKO0k7bf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DenseLayer:\n",
        "  def __init__(self, n_inputs, n_neurons):\n",
        "    \"\"\"\n",
        "    Initialize weights & biases.\n",
        "    Weights should be initialized with values drawn from a normal distribution scaled by 0.01.\n",
        "    Biases are initialized to 0.0.\n",
        "    \"\"\"\n",
        "    self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
        "    self.biases = np.zeros((1, n_neurons))\n",
        "\n",
        "  def forward(self, inputs):\n",
        "    \"\"\"\n",
        "    A forward pass through the layer to give z.\n",
        "    Compute it using np.dot(...) and then add the biases.\n",
        "    \"\"\"\n",
        "    self.inputs = inputs\n",
        "    self.z = np.dot(inputs, self.weights) + self.biases\n",
        "\n",
        "  def backward(self, dz):\n",
        "    \"\"\"\n",
        "    Backward pass\n",
        "    \"\"\"\n",
        "    # Gradients of weights = dot product(transpose inputs, gradient descent z)\n",
        "    self.dweights = np.dot(self.inputs.T, dz)\n",
        "    # Gradients of biases\n",
        "    self.dbiases = np.sum(dz, axis=0, keepdims=True)\n",
        "    # Gradients of inputs\n",
        "    self.dinputs = np.dot(dz, self.weights.T)\n"
      ],
      "metadata": {
        "id": "iadz9pEtlARH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Activations\n",
        "- ReLu"
      ],
      "metadata": {
        "id": "2ojUK5CCsImN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ReLu:\n",
        "    \"\"\"\n",
        "    ReLu activation\n",
        "    \"\"\"\n",
        "    def forward(self, z):\n",
        "      \"\"\"\n",
        "      Forward pass\n",
        "      \"\"\"\n",
        "      self.z = z\n",
        "      self.activity = np.maximum(0, z)\n",
        "\n",
        "\n",
        "    def backward(self, dactivity):\n",
        "      \"\"\"\n",
        "      Backward pass\n",
        "      \"\"\"\n",
        "      self.dz = dactivity.copy()\n",
        "      self.dz[self.z <= 0] = 0.0\n",
        "      return self.dz"
      ],
      "metadata": {
        "id": "UuiRCxPssJA7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Softmax"
      ],
      "metadata": {
        "id": "lGZRZIHQ01p7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Softmax:\n",
        "    def forward(self, z):\n",
        "        \"\"\"\n",
        "        Compute the softmax of vector z in a numerically stable way,\n",
        "        and save the computed probabilities as an attribute for backward pass.\n",
        "        \"\"\"\n",
        "        shift_z = z - np.max(z, axis=1, keepdims=True)\n",
        "        self.probs = np.exp(shift_z) / np.sum(np.exp(shift_z), axis=1, keepdims=True)\n",
        "        return self.probs\n",
        "\n",
        "    def backward(self, dprobs):\n",
        "        \"\"\"\n",
        "        Compute the gradient of the loss with respect to z,\n",
        "        using stored probs from the forward pass.\n",
        "        \"\"\"\n",
        "        self.dz = np.empty_like(dprobs)\n",
        "        for i, (prob, dprob) in enumerate(zip(self.probs, dprobs)):\n",
        "            prob = prob.reshape(-1, 1)\n",
        "            jacobian = np.diagflat(prob) - np.dot(prob, prob.T)\n",
        "            self.dz[i] = np.dot(jacobian, dprob)\n",
        "        return self.dz"
      ],
      "metadata": {
        "id": "SBThvBkBnNvh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loss function\n",
        "- Crossentropy loss"
      ],
      "metadata": {
        "id": "BYnzJ2-nL5Yh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CrossEntropyLoss:\n",
        "    def forward(self, probs, oh_y_true):\n",
        "        probs_clipped = np.clip(probs, 1e-7, 1 - 1e-7)\n",
        "        loss = -np.sum(oh_y_true * np.log(probs_clipped), axis=1)\n",
        "        return loss.mean(axis=0)\n",
        "\n",
        "    def backward(self, probs, oh_y_true):\n",
        "        batch_sz, n_class = probs.shape\n",
        "        self.dprobs = -oh_y_true / probs\n",
        "        self.dprobs = self.dprobs / batch_sz\n",
        "        return self.dprobs"
      ],
      "metadata": {
        "id": "8VOHxTIuL3mw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optimizer\n",
        "- Stochastic Gradient Descent\n",
        "\n",
        "Note that this optimizer only updates a single layer, and have thus, to be called for each layer."
      ],
      "metadata": {
        "id": "qHP_WUAgMEGF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SGD:\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "    def __init__(self, learning_rate=1.0):\n",
        "      # Initialize the optimizer with a learning rate\n",
        "      self.learning_rate = learning_rate\n",
        "\n",
        "    def update_params(self, layer, dweights, dbiases):\n",
        "      layer.weights -= self.learning_rate * dweights\n",
        "      layer.biases -= self.learning_rate * dbiases"
      ],
      "metadata": {
        "id": "TNG5P6yyMPfu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Helper functions\n",
        "- Convert probabilities to predictions"
      ],
      "metadata": {
        "id": "sQFWtoYnPWjk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predictions(probs):\n",
        "  \"\"\"\n",
        "  \"\"\"\n",
        "  y_preds = np.argmax(probs, axis=1)\n",
        "  return y_preds"
      ],
      "metadata": {
        "id": "qYV0OuJwPUx-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Accuracy\n"
      ],
      "metadata": {
        "id": "EzcNoTThMlE0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy(y_preds, y_true):\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "    return np.mean(y_preds == y_true)"
      ],
      "metadata": {
        "id": "WqL21POjMoyw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- One-hot encoding"
      ],
      "metadata": {
        "id": "fyJ3-JoYMugH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# oh_y_true = np.eye(n_class)[y_true]; n_class = 3"
      ],
      "metadata": {
        "id": "wbg1hki7MsgZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training\n",
        "- A single forward pass through the entire network.\n",
        "\n"
      ],
      "metadata": {
        "id": "pbJHIPFBQYqd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def forward_pass(X, y_true, oh_y_true):\n",
        "    # logits for the first hidden layer\n",
        "    dense1.forward(X)\n",
        "    # ReLU activation apply to the first hidden layer logits\n",
        "    activation1.forward(dense1.z)\n",
        "\n",
        "    # logits for the second hidden layer using first layer's activated output\n",
        "    dense2.forward(activation1.activity)\n",
        "    # ReLU activation apply to the second hidden layer logits\n",
        "    activation2.forward(dense2.z)\n",
        "\n",
        "    dense_output.forward(activation2.activity)\n",
        "    # convert output layer logits to probabilities using softmax activation\n",
        "    probs = Softmax().forward(dense_output.z)\n",
        "\n",
        "    return probs"
      ],
      "metadata": {
        "id": "nejtC2eJnSXB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- A single backward pass through the entire network.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-ythQ3QLQg2N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def backward_pass(probs, y_true, oh_y_true):\n",
        "    dprobs = crossentropy.backward(probs, oh_y_true)\n",
        "\n",
        "    dactivation_output = Softmax().backward(dprobs)\n",
        "    dense_output.backward(dactivation_output)\n",
        "\n",
        "    dactivation2 = activation2.backward(dense_output.dinputs)\n",
        "    dense2.backward(dactivation2)\n",
        "\n",
        "    dactivation1 = activation1.backward(dense2.dinputs)\n",
        "    dense1.backward(dactivation1)\n",
        "\n",
        "    return dense1.dweights, dense1.dbiases, dense2.dweights, dense2.dbiases,\n",
        "    dense_output.dweights, dense_output.dbiases\n"
      ],
      "metadata": {
        "id": "4sZQj4eXj-eI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Initialize the network and set hyperparameters\n",
        "\n",
        "For example, number of\n",
        "epochs to train, batch size, number of neurons, etc."
      ],
      "metadata": {
        "id": "Xbh62dvcQmQY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 9\n",
        "\n",
        "dense1 = DenseLayer(n_inputs=3, n_neurons=4)\n",
        "dense2 = DenseLayer(n_inputs=4, n_neurons=8)\n",
        "dense_output = DenseLayer(n_inputs=8, n_neurons=3)\n",
        "\n",
        "activation1 = ReLu()\n",
        "activation2 = ReLu()\n",
        "\n",
        "n_batch = 64\n",
        "output_activation = Softmax()\n",
        "crossentropy = CrossEntropyLoss()\n",
        "optimizer = SGD()"
      ],
      "metadata": {
        "id": "EMfyy4WVQ5Gl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training loop"
      ],
      "metadata": {
        "id": "luYRj0d3RE-s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(epochs):\n",
        "    print('epoch:', epoch)\n",
        "\n",
        "    for i in range(0, len(X_train), n_batch):\n",
        "        # Get a mini-batch of data from X_train and y_train.\n",
        "        # It should have batch_sz examples.\n",
        "        X_mini_batch = X_train[i:i+n_batch]\n",
        "        y_mini_batch = y_train[i:i+n_batch]\n",
        "\n",
        "        # One-hot encode y_true\n",
        "        oh_y_true_mini_batch = np.eye(3)[y_mini_batch]\n",
        "\n",
        "        # Forward pass\n",
        "        dense1.forward(X_mini_batch)\n",
        "        activation1.forward(dense1.z)\n",
        "        dense2.forward(activation1.activity)\n",
        "        activation2.forward(dense2.z)\n",
        "        dense_output.forward(activation2.activity)\n",
        "        probs = output_activation.forward(dense_output.z)\n",
        "\n",
        "        # Loss\n",
        "        loss = crossentropy.forward(probs, oh_y_true_mini_batch)\n",
        "\n",
        "        # Print accuracy and loss\n",
        "        y_preds = predictions(probs)\n",
        "        acc = accuracy(y_preds, y_mini_batch)\n",
        "        print(f\"Batch {i//n_batch+1} - Loss: {loss}, Accuracy: {acc}\")\n",
        "\n",
        "        # Backward pass\n",
        "        dprobs = crossentropy.backward(probs, oh_y_true_mini_batch)\n",
        "        dactivation_output = output_activation.backward(dprobs)\n",
        "        dense_output.backward(dactivation_output)\n",
        "        dactivation2 = activation2.backward(dense_output.dinputs)\n",
        "        dense2.backward(dactivation2)\n",
        "        dactivation1 = activation1.backward(dense2.dinputs)\n",
        "        dense1.backward(dactivation1)\n",
        "\n",
        "        # Update weights and biases\n",
        "        optimizer.update_params(dense1, dense1.dweights, dense1.dbiases)\n",
        "        optimizer.update_params(dense2, dense2.dweights, dense2.dbiases)\n",
        "        optimizer.update_params(dense_output, dense_output.dweights,\n",
        "                                dense_output.dbiases)\n"
      ],
      "metadata": {
        "id": "i8I-SrJbpRbs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4bc589f3-b044-46c6-c721-e8b4e7da5b9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 0\n",
            "Batch 1 - Loss: 1.098613134948772, Accuracy: 0.328125\n",
            "Batch 2 - Loss: 1.0894391271203354, Accuracy: 0.421875\n",
            "Batch 3 - Loss: 1.113779887801104, Accuracy: 0.265625\n",
            "Batch 4 - Loss: 1.0948815830304692, Accuracy: 0.375\n",
            "Batch 5 - Loss: 1.1092432860844088, Accuracy: 0.28125\n",
            "Batch 6 - Loss: 1.098826395759021, Accuracy: 0.375\n",
            "Batch 7 - Loss: 1.1013595995842869, Accuracy: 0.25\n",
            "epoch: 1\n",
            "Batch 1 - Loss: 1.1102656840966079, Accuracy: 0.28125\n",
            "Batch 2 - Loss: 1.099654014969227, Accuracy: 0.421875\n",
            "Batch 3 - Loss: 1.1110082564485524, Accuracy: 0.265625\n",
            "Batch 4 - Loss: 1.0957918170736884, Accuracy: 0.375\n",
            "Batch 5 - Loss: 1.1067140046914388, Accuracy: 0.28125\n",
            "Batch 6 - Loss: 1.0980015389399265, Accuracy: 0.375\n",
            "Batch 7 - Loss: 1.0998740862955736, Accuracy: 0.25\n",
            "epoch: 2\n",
            "Batch 1 - Loss: 1.1111997207998239, Accuracy: 0.28125\n",
            "Batch 2 - Loss: 1.1003626589189142, Accuracy: 0.234375\n",
            "Batch 3 - Loss: 1.110886494881019, Accuracy: 0.265625\n",
            "Batch 4 - Loss: 1.0958635280092843, Accuracy: 0.375\n",
            "Batch 5 - Loss: 1.1065658966381116, Accuracy: 0.28125\n",
            "Batch 6 - Loss: 1.0979510450980574, Accuracy: 0.375\n",
            "Batch 7 - Loss: 1.0997852793296425, Accuracy: 0.25\n",
            "epoch: 3\n",
            "Batch 1 - Loss: 1.111251477143149, Accuracy: 0.28125\n",
            "Batch 2 - Loss: 1.100400809651811, Accuracy: 0.234375\n",
            "Batch 3 - Loss: 1.1108749323131046, Accuracy: 0.265625\n",
            "Batch 4 - Loss: 1.0958629485392044, Accuracy: 0.375\n",
            "Batch 5 - Loss: 1.1065542085077107, Accuracy: 0.28125\n",
            "Batch 6 - Loss: 1.0979436782979966, Accuracy: 0.375\n",
            "Batch 7 - Loss: 1.099774837168398, Accuracy: 0.25\n",
            "epoch: 4\n",
            "Batch 1 - Loss: 1.1112488455943619, Accuracy: 0.28125\n",
            "Batch 2 - Loss: 1.1003970920362682, Accuracy: 0.234375\n",
            "Batch 3 - Loss: 1.1108667676153456, Accuracy: 0.265625\n",
            "Batch 4 - Loss: 1.095855168712398, Accuracy: 0.375\n",
            "Batch 5 - Loss: 1.1065485778678643, Accuracy: 0.28125\n",
            "Batch 6 - Loss: 1.0979360665078972, Accuracy: 0.375\n",
            "Batch 7 - Loss: 1.0997656346262743, Accuracy: 0.25\n",
            "epoch: 5\n",
            "Batch 1 - Loss: 1.111239249552547, Accuracy: 0.28125\n",
            "Batch 2 - Loss: 1.1003868756237671, Accuracy: 0.234375\n",
            "Batch 3 - Loss: 1.1108537613329117, Accuracy: 0.265625\n",
            "Batch 4 - Loss: 1.0958415467733693, Accuracy: 0.375\n",
            "Batch 5 - Loss: 1.1065391111645992, Accuracy: 0.28125\n",
            "Batch 6 - Loss: 1.0979229215322586, Accuracy: 0.375\n",
            "Batch 7 - Loss: 1.099750600248379, Accuracy: 0.25\n",
            "epoch: 6\n",
            "Batch 1 - Loss: 1.111220294091058, Accuracy: 0.28125\n",
            "Batch 2 - Loss: 1.1003675914375193, Accuracy: 0.234375\n",
            "Batch 3 - Loss: 1.110829646074424, Accuracy: 0.265625\n",
            "Batch 4 - Loss: 1.0958160080819281, Accuracy: 0.375\n",
            "Batch 5 - Loss: 1.1065205610748938, Accuracy: 0.28125\n",
            "Batch 6 - Loss: 1.0978981387804057, Accuracy: 0.375\n",
            "Batch 7 - Loss: 1.099720663664982, Accuracy: 0.25\n",
            "epoch: 7\n",
            "Batch 1 - Loss: 1.1111847220752706, Accuracy: 0.28125\n",
            "Batch 2 - Loss: 1.1003288784561849, Accuracy: 0.234375\n",
            "Batch 3 - Loss: 1.1107808747285586, Accuracy: 0.265625\n",
            "Batch 4 - Loss: 1.0957631773786076, Accuracy: 0.375\n",
            "Batch 5 - Loss: 1.1064779929482733, Accuracy: 0.28125\n",
            "Batch 6 - Loss: 1.0978438890857647, Accuracy: 0.375\n",
            "Batch 7 - Loss: 1.0996564552571702, Accuracy: 0.25\n",
            "epoch: 8\n",
            "Batch 1 - Loss: 1.1111006542005444, Accuracy: 0.28125\n",
            "Batch 2 - Loss: 1.1002380925345383, Accuracy: 0.234375\n",
            "Batch 3 - Loss: 1.1106626703933757, Accuracy: 0.265625\n",
            "Batch 4 - Loss: 1.0956337753901177, Accuracy: 0.375\n",
            "Batch 5 - Loss: 1.1063652785453535, Accuracy: 0.28125\n",
            "Batch 6 - Loss: 1.0977017924527754, Accuracy: 0.375\n",
            "Batch 7 - Loss: 1.0994865204403461, Accuracy: 0.25\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task\n",
        "Use the code provided below to complete and train the network. You should get an accuracy above 90% in less than 10 epochs of training.\n",
        "\n"
      ],
      "metadata": {
        "id": "AfNhVPbmkttf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Forward pass\n",
        "dense1.forward(X_test)\n",
        "activation1.forward(dense1.z)\n",
        "dense2.forward(activation1.activity)\n",
        "activation2.forward(dense2.z)\n",
        "dense_output.forward(activation2.activity)\n",
        "probs = output_activation.forward(dense_output.z)\n",
        "\n",
        "# Compute loss and accuracy\n",
        "loss = crossentropy.forward(probs, np.eye(3)[y_test])\n",
        "y_preds = predictions(probs)\n",
        "acc = accuracy(y_preds, y_test)\n",
        "\n",
        "print(f\"Test Loss: {loss}, Test Accuracy: {acc}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t_GYxldBrQPI",
        "outputId": "213baf35-4612-460b-ab17-1f8ea31ccef3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 1.0974891529758646, Test Accuracy: 0.36\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parameter Tuning"
      ],
      "metadata": {
        "id": "6AIL9mus7ofz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Grid Search"
      ],
      "metadata": {
        "id": "9WXySXiwAcQz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def validate_model(X_val, y_val):\n",
        "    # Forward pass through the network\n",
        "    dense1.forward(X_val)\n",
        "    activation1.forward(dense1.z)\n",
        "    dense2.forward(activation1.activity)\n",
        "    activation2.forward(dense2.z)\n",
        "    dense_output.forward(activation2.activity)\n",
        "    probs = output_activation.forward(dense_output.z)\n",
        "\n",
        "    y_preds = predictions(probs)\n",
        "    acc = accuracy(y_preds, y_val)\n",
        "\n",
        "    return acc"
      ],
      "metadata": {
        "id": "eySl43VY-A4f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_val_samples = int(0.6 * len(X_train))\n",
        "\n",
        "# Shuffle data\n",
        "indices = np.arange(len(X_train))\n",
        "np.random.shuffle(indices)\n",
        "\n",
        "X_train = X_train[indices]\n",
        "y_train = y_train[indices]\n",
        "\n",
        "# Split into training and validation sets\n",
        "X_val = X_train[:n_val_samples]\n",
        "y_val = y_train[:n_val_samples]\n",
        "X_train = X_train[n_val_samples:]\n",
        "y_train = y_train[n_val_samples:]"
      ],
      "metadata": {
        "id": "2FbYOvNN-b0Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Find the best hyperparameters"
      ],
      "metadata": {
        "id": "L6wVns-1UU89"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define hyperparameter space\n",
        "epochs_space = [5,6,7, 8, 9, 10]\n",
        "batch_size_space = np.random.randint(1, 101, size=10).tolist()\n",
        "\n",
        "# Best parameters and accuracy initialization\n",
        "best_params = None\n",
        "best_acc = 0\n",
        "\n",
        "# Grid search\n",
        "for epochs in epochs_space:\n",
        "    for batch_size in batch_size_space:\n",
        "\n",
        "        # Training loop\n",
        "        for epoch in range(epochs):\n",
        "          for i in range(0, len(X_train), batch_size):\n",
        "            X_mini_batch = X_train[i:i+batch_size]\n",
        "            y_mini_batch = y_train[i:i+batch_size]\n",
        "\n",
        "            # One-hot encode y_true\n",
        "            oh_y_true_mini_batch = np.eye(3)[y_mini_batch]\n",
        "\n",
        "            # Forward pass\n",
        "            dense1.forward(X_mini_batch)\n",
        "            activation1.forward(dense1.z)\n",
        "            dense2.forward(activation1.activity)\n",
        "            activation2.forward(dense2.z)\n",
        "            dense_output.forward(activation2.activity)\n",
        "            probs = output_activation.forward(dense_output.z)\n",
        "\n",
        "            # Loss\n",
        "            loss = crossentropy.forward(probs, oh_y_true_mini_batch)\n",
        "\n",
        "            # Print accuracy and loss\n",
        "            y_preds = predictions(probs)\n",
        "            acc = accuracy(y_preds, y_mini_batch)\n",
        "            print(f\"Batch {i//n_batch+1} - Loss: {loss}, Accuracy: {acc}\")\n",
        "\n",
        "            # Backward pass\n",
        "            dprobs = crossentropy.backward(probs, oh_y_true_mini_batch)\n",
        "            dactivation_output = output_activation.backward(dprobs)\n",
        "            dense_output.backward(dactivation_output)\n",
        "            dactivation2 = activation2.backward(dense_output.dinputs)\n",
        "            dense2.backward(dactivation2)\n",
        "            dactivation1 = activation1.backward(dense2.dinputs)\n",
        "            dense1.backward(dactivation1)\n",
        "\n",
        "            # Update weights and biases\n",
        "            optimizer.update_params(dense1, dense1.dweights, dense1.dbiases)\n",
        "            optimizer.update_params(dense2, dense2.dweights, dense2.dbiases)\n",
        "            optimizer.update_params(dense_output, dense_output.dweights,\n",
        "                                dense_output.dbiases)\n",
        "\n",
        "        # Validate the model\n",
        "        val_acc = validate_model(X_val, y_val)\n",
        "\n",
        "        # Update best parameters if current model is better\n",
        "        if val_acc > best_acc:\n",
        "            best_acc = val_acc\n",
        "            best_params = (epochs, batch_size)\n",
        "\n",
        "# Use best_params for further training/testing\n",
        "best_epochs, best_batch_size = best_params"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kaKKhtD37qw0",
        "outputId": "a61faaaa-5f5a-4bbd-a396-fb05a1e050c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 1 - Loss: 1.1126193706775842, Accuracy: 0.273972602739726\n",
            "Batch 2 - Loss: 1.0977505898668167, Accuracy: 0.3424657534246575\n",
            "Batch 3 - Loss: 1.0906385731855583, Accuracy: 0.5714285714285714\n",
            "Batch 1 - Loss: 1.1372550656083489, Accuracy: 0.273972602739726\n",
            "Batch 2 - Loss: 1.098994004077133, Accuracy: 0.3424657534246575\n",
            "Batch 3 - Loss: 1.072136083351216, Accuracy: 0.5714285714285714\n",
            "Batch 1 - Loss: 1.1454636230763149, Accuracy: 0.273972602739726\n",
            "Batch 2 - Loss: 1.099698386911617, Accuracy: 0.3424657534246575\n",
            "Batch 3 - Loss: 1.0671713655483839, Accuracy: 0.5714285714285714\n",
            "Batch 1 - Loss: 1.147451386361045, Accuracy: 0.273972602739726\n",
            "Batch 2 - Loss: 1.099398656495654, Accuracy: 0.3424657534246575\n",
            "Batch 3 - Loss: 1.0654975329154863, Accuracy: 0.5714285714285714\n",
            "Batch 1 - Loss: 1.147221097922795, Accuracy: 0.273972602739726\n",
            "Batch 2 - Loss: 1.0981950178262834, Accuracy: 0.3424657534246575\n",
            "Batch 3 - Loss: 1.0641187989127565, Accuracy: 0.5714285714285714\n",
            "Batch 1 - Loss: 1.1703857615044981, Accuracy: 0.21212121212121213\n",
            "Batch 1 - Loss: 1.1146103661768143, Accuracy: 0.36363636363636365\n",
            "Batch 2 - Loss: 1.092848624285045, Accuracy: 0.3333333333333333\n",
            "Batch 2 - Loss: 1.0935729880402054, Accuracy: 0.42424242424242425\n",
            "Batch 3 - Loss: 1.0909673322036884, Accuracy: 0.5\n",
            "Batch 1 - Loss: 1.100509249548569, Accuracy: 0.21212121212121213\n",
            "Batch 1 - Loss: 1.0819777688331804, Accuracy: 0.21212121212121213\n",
            "Batch 2 - Loss: 1.0282809897399443, Accuracy: 0.42424242424242425\n",
            "Batch 2 - Loss: 0.967087811837803, Accuracy: 0.5757575757575758\n",
            "Batch 3 - Loss: 0.9620071139082794, Accuracy: 0.5\n",
            "Batch 1 - Loss: 0.8114058235295147, Accuracy: 0.5151515151515151\n",
            "Batch 1 - Loss: 0.6450901961689267, Accuracy: 0.6060606060606061\n",
            "Batch 2 - Loss: 0.6564353814027296, Accuracy: 0.6363636363636364\n",
            "Batch 2 - Loss: 0.6119594957476202, Accuracy: 0.6060606060606061\n",
            "Batch 3 - Loss: 0.6994906788084353, Accuracy: 0.5\n",
            "Batch 1 - Loss: 0.6209644177136194, Accuracy: 0.5151515151515151\n",
            "Batch 1 - Loss: 0.5498798622871153, Accuracy: 0.6060606060606061\n",
            "Batch 2 - Loss: 0.5560091350692571, Accuracy: 0.6363636363636364\n",
            "Batch 2 - Loss: 0.5306151628825102, Accuracy: 0.6060606060606061\n",
            "Batch 3 - Loss: 0.6673355244213208, Accuracy: 0.5\n",
            "Batch 1 - Loss: 0.6335233911720292, Accuracy: 0.5151515151515151\n",
            "Batch 1 - Loss: 0.543446552103263, Accuracy: 0.6060606060606061\n",
            "Batch 2 - Loss: 0.541672505806606, Accuracy: 0.6363636363636364\n",
            "Batch 2 - Loss: 0.5038019973673915, Accuracy: 0.7272727272727273\n",
            "Batch 3 - Loss: 0.6510207285506773, Accuracy: 0.5\n",
            "Batch 1 - Loss: 0.5325476478776743, Accuracy: 0.6086956521739131\n",
            "Batch 1 - Loss: 0.4452579616969946, Accuracy: 0.6956521739130435\n",
            "Batch 2 - Loss: 0.48529308686530276, Accuracy: 0.7608695652173914\n",
            "Batch 3 - Loss: 0.6465131145531777, Accuracy: 0.5454545454545454\n",
            "Batch 1 - Loss: 0.527674288049458, Accuracy: 0.6086956521739131\n",
            "Batch 1 - Loss: 0.3986287598072842, Accuracy: 0.8478260869565217\n",
            "Batch 2 - Loss: 0.3836083687731961, Accuracy: 0.8695652173913043\n",
            "Batch 3 - Loss: 0.5637025397606914, Accuracy: 0.8181818181818182\n",
            "Batch 1 - Loss: 0.436096047711441, Accuracy: 0.8043478260869565\n",
            "Batch 1 - Loss: 0.26739887121422246, Accuracy: 0.9130434782608695\n",
            "Batch 2 - Loss: 0.23743164664233363, Accuracy: 0.9782608695652174\n",
            "Batch 3 - Loss: 0.42216340124584745, Accuracy: 0.9090909090909091\n",
            "Batch 1 - Loss: 0.29273135235967374, Accuracy: 0.9347826086956522\n",
            "Batch 1 - Loss: 0.17151414847976082, Accuracy: 0.9782608695652174\n",
            "Batch 2 - Loss: 0.15482142974318383, Accuracy: 1.0\n",
            "Batch 3 - Loss: 0.3269256574714267, Accuracy: 0.9090909090909091\n",
            "Batch 1 - Loss: 0.22265833218939718, Accuracy: 0.9130434782608695\n",
            "Batch 1 - Loss: 0.16008823794438967, Accuracy: 0.9565217391304348\n",
            "Batch 2 - Loss: 0.1235556126600296, Accuracy: 1.0\n",
            "Batch 3 - Loss: 0.29029712541188535, Accuracy: 0.9090909090909091\n",
            "Batch 1 - Loss: 0.20054074195893357, Accuracy: 0.9393939393939394\n",
            "Batch 2 - Loss: 0.1380551039644749, Accuracy: 0.9545454545454546\n",
            "Batch 3 - Loss: 0.32479641483270605, Accuracy: 0.8928571428571429\n",
            "Batch 1 - Loss: 0.3026893841275673, Accuracy: 0.9242424242424242\n",
            "Batch 2 - Loss: 0.15130287311102653, Accuracy: 0.9545454545454546\n",
            "Batch 3 - Loss: 0.3209602580826146, Accuracy: 0.9285714285714286\n",
            "Batch 1 - Loss: 0.22517378906952112, Accuracy: 0.9393939393939394\n",
            "Batch 2 - Loss: 0.160782921609519, Accuracy: 0.9393939393939394\n",
            "Batch 3 - Loss: 0.28898660647351837, Accuracy: 0.9285714285714286\n",
            "Batch 1 - Loss: 0.20123705425044913, Accuracy: 0.9393939393939394\n",
            "Batch 2 - Loss: 0.1448896567786132, Accuracy: 0.9393939393939394\n",
            "Batch 3 - Loss: 0.2761725748279717, Accuracy: 0.9285714285714286\n",
            "Batch 1 - Loss: 0.19044144110854413, Accuracy: 0.9393939393939394\n",
            "Batch 2 - Loss: 0.14235493605545066, Accuracy: 0.9393939393939394\n",
            "Batch 3 - Loss: 0.2649908578543195, Accuracy: 0.9285714285714286\n",
            "Batch 1 - Loss: 0.2138632692013647, Accuracy: 0.9210526315789473\n",
            "Batch 2 - Loss: 0.12787640477614784, Accuracy: 0.9473684210526315\n",
            "Batch 3 - Loss: 0.24595686918481502, Accuracy: 0.875\n",
            "Batch 1 - Loss: 0.300660717167858, Accuracy: 0.9210526315789473\n",
            "Batch 2 - Loss: 0.12548265516011964, Accuracy: 0.9473684210526315\n",
            "Batch 3 - Loss: 0.23256045948060983, Accuracy: 0.875\n",
            "Batch 1 - Loss: 0.2790130428954429, Accuracy: 0.9210526315789473\n",
            "Batch 2 - Loss: 0.1379516684915635, Accuracy: 0.9342105263157895\n",
            "Batch 3 - Loss: 0.2323303139613945, Accuracy: 0.875\n",
            "Batch 1 - Loss: 0.2817349653200903, Accuracy: 0.9210526315789473\n",
            "Batch 2 - Loss: 0.1524758895900049, Accuracy: 0.9342105263157895\n",
            "Batch 3 - Loss: 0.22571235626238143, Accuracy: 0.875\n",
            "Batch 1 - Loss: 0.2700148517728121, Accuracy: 0.9210526315789473\n",
            "Batch 2 - Loss: 0.14798867267115873, Accuracy: 0.9342105263157895\n",
            "Batch 3 - Loss: 0.21719495860785723, Accuracy: 0.875\n",
            "Batch 1 - Loss: 0.19981572370205322, Accuracy: 0.9428571428571428\n",
            "Batch 1 - Loss: 0.21099254084025035, Accuracy: 0.9428571428571428\n",
            "Batch 2 - Loss: 0.20291489099240095, Accuracy: 0.8857142857142857\n",
            "Batch 2 - Loss: 0.07153626201243472, Accuracy: 0.9714285714285714\n",
            "Batch 3 - Loss: 0.2729215374771701, Accuracy: 0.9\n",
            "Batch 1 - Loss: 0.20295127198821553, Accuracy: 0.9428571428571428\n",
            "Batch 1 - Loss: 0.19754007291758358, Accuracy: 0.9428571428571428\n",
            "Batch 2 - Loss: 0.20271641261983775, Accuracy: 0.8857142857142857\n",
            "Batch 2 - Loss: 0.06780889794910674, Accuracy: 1.0\n",
            "Batch 3 - Loss: 0.2762502397954223, Accuracy: 0.9\n",
            "Batch 1 - Loss: 0.20126531296123984, Accuracy: 0.9428571428571428\n",
            "Batch 1 - Loss: 0.19217050148735978, Accuracy: 0.9428571428571428\n",
            "Batch 2 - Loss: 0.20100597653139163, Accuracy: 0.8857142857142857\n",
            "Batch 2 - Loss: 0.0654578498226312, Accuracy: 1.0\n",
            "Batch 3 - Loss: 0.27712129825819837, Accuracy: 0.9\n",
            "Batch 1 - Loss: 0.20395680739857341, Accuracy: 0.9428571428571428\n",
            "Batch 1 - Loss: 0.18654244598801972, Accuracy: 0.9428571428571428\n",
            "Batch 2 - Loss: 0.20255618775485154, Accuracy: 0.8857142857142857\n",
            "Batch 2 - Loss: 0.0627350276037292, Accuracy: 1.0\n",
            "Batch 3 - Loss: 0.27909957731553686, Accuracy: 0.9\n",
            "Batch 1 - Loss: 0.19949118471774355, Accuracy: 0.9428571428571428\n",
            "Batch 1 - Loss: 0.1806840619810982, Accuracy: 0.9428571428571428\n",
            "Batch 2 - Loss: 0.202192745944432, Accuracy: 0.8857142857142857\n",
            "Batch 2 - Loss: 0.06091460367468856, Accuracy: 1.0\n",
            "Batch 3 - Loss: 0.28189633882864173, Accuracy: 0.9\n",
            "Batch 1 - Loss: 0.2509142069391844, Accuracy: 0.9186046511627907\n",
            "Batch 2 - Loss: 0.13089716582721533, Accuracy: 0.9324324324324325\n",
            "Batch 1 - Loss: 0.13260982782473454, Accuracy: 0.9534883720930233\n",
            "Batch 2 - Loss: 0.10365607942597065, Accuracy: 0.9459459459459459\n",
            "Batch 1 - Loss: 0.13121250809377236, Accuracy: 0.9534883720930233\n",
            "Batch 2 - Loss: 0.10261852503947222, Accuracy: 0.9459459459459459\n",
            "Batch 1 - Loss: 0.13006408288947305, Accuracy: 0.9534883720930233\n",
            "Batch 2 - Loss: 0.10168872728823775, Accuracy: 0.9459459459459459\n",
            "Batch 1 - Loss: 0.12895334264702088, Accuracy: 0.9534883720930233\n",
            "Batch 2 - Loss: 0.10097990552154573, Accuracy: 0.9459459459459459\n",
            "Batch 1 - Loss: 0.12658315002818815, Accuracy: 0.9540229885057471\n",
            "Batch 2 - Loss: 0.10179847690412729, Accuracy: 0.9452054794520548\n",
            "Batch 1 - Loss: 0.12586307763889998, Accuracy: 0.9540229885057471\n",
            "Batch 2 - Loss: 0.10106042078459138, Accuracy: 0.9452054794520548\n",
            "Batch 1 - Loss: 0.12511188394732004, Accuracy: 0.9540229885057471\n",
            "Batch 2 - Loss: 0.10055735234800979, Accuracy: 0.9452054794520548\n",
            "Batch 1 - Loss: 0.12452187405747701, Accuracy: 0.9540229885057471\n",
            "Batch 2 - Loss: 0.09990967911510251, Accuracy: 0.9452054794520548\n",
            "Batch 1 - Loss: 0.12383990391745427, Accuracy: 0.9540229885057471\n",
            "Batch 2 - Loss: 0.09940341676380517, Accuracy: 0.9452054794520548\n",
            "Batch 1 - Loss: 0.11510300848362393, Accuracy: 0.9534883720930233\n",
            "Batch 1 - Loss: 0.15853346085283523, Accuracy: 0.9302325581395349\n",
            "Batch 2 - Loss: 0.047831038069784906, Accuracy: 1.0\n",
            "Batch 3 - Loss: 0.2450995443571983, Accuracy: 0.9354838709677419\n",
            "Batch 1 - Loss: 0.12512124888587858, Accuracy: 0.9534883720930233\n",
            "Batch 1 - Loss: 0.1848383241857861, Accuracy: 0.9069767441860465\n",
            "Batch 2 - Loss: 0.05500410968893073, Accuracy: 0.9767441860465116\n",
            "Batch 3 - Loss: 0.2421426433394161, Accuracy: 0.9354838709677419\n",
            "Batch 1 - Loss: 0.12160574321191238, Accuracy: 0.9534883720930233\n",
            "Batch 1 - Loss: 0.18354700566266793, Accuracy: 0.9069767441860465\n",
            "Batch 2 - Loss: 0.05629878795329335, Accuracy: 0.9767441860465116\n",
            "Batch 3 - Loss: 0.2438359644503679, Accuracy: 0.9354838709677419\n",
            "Batch 1 - Loss: 0.12042916767185251, Accuracy: 0.9534883720930233\n",
            "Batch 1 - Loss: 0.1814275890456913, Accuracy: 0.9069767441860465\n",
            "Batch 2 - Loss: 0.05738814287704695, Accuracy: 0.9767441860465116\n",
            "Batch 3 - Loss: 0.24314324206472251, Accuracy: 0.9354838709677419\n",
            "Batch 1 - Loss: 0.11903379566767087, Accuracy: 0.9534883720930233\n",
            "Batch 1 - Loss: 0.17885179113360233, Accuracy: 0.9069767441860465\n",
            "Batch 2 - Loss: 0.05804453685439473, Accuracy: 0.9767441860465116\n",
            "Batch 3 - Loss: 0.24021895681159885, Accuracy: 0.9354838709677419\n",
            "Batch 1 - Loss: 0.16475991452208769, Accuracy: 0.9361702127659575\n",
            "Batch 1 - Loss: 0.11114137840880933, Accuracy: 0.9361702127659575\n",
            "Batch 2 - Loss: 0.08400040002336918, Accuracy: 0.9574468085106383\n",
            "Batch 3 - Loss: 0.3874325319585301, Accuracy: 0.8947368421052632\n",
            "Batch 1 - Loss: 0.26992861667073664, Accuracy: 0.8936170212765957\n",
            "Batch 1 - Loss: 0.0634067228458414, Accuracy: 0.9787234042553191\n",
            "Batch 2 - Loss: 0.05223902053401657, Accuracy: 1.0\n",
            "Batch 3 - Loss: 0.47723757409437656, Accuracy: 0.8947368421052632\n",
            "Batch 1 - Loss: 0.2793569768169055, Accuracy: 0.9148936170212766\n",
            "Batch 1 - Loss: 0.12551900621076195, Accuracy: 0.9361702127659575\n",
            "Batch 2 - Loss: 0.08899663584766096, Accuracy: 0.9574468085106383\n",
            "Batch 3 - Loss: 0.502112575876461, Accuracy: 0.8947368421052632\n",
            "Batch 1 - Loss: 0.26704103821334, Accuracy: 0.9148936170212766\n",
            "Batch 1 - Loss: 0.11859816350184103, Accuracy: 0.9361702127659575\n",
            "Batch 2 - Loss: 0.0937103794177936, Accuracy: 0.9574468085106383\n",
            "Batch 3 - Loss: 0.4725138246480524, Accuracy: 0.8947368421052632\n",
            "Batch 1 - Loss: 0.2552532484462512, Accuracy: 0.9148936170212766\n",
            "Batch 1 - Loss: 0.12048318824694876, Accuracy: 0.9361702127659575\n",
            "Batch 2 - Loss: 0.10115204342886161, Accuracy: 0.9574468085106383\n",
            "Batch 3 - Loss: 0.44612821355364957, Accuracy: 0.8947368421052632\n",
            "Batch 1 - Loss: 0.2406304449168604, Accuracy: 0.9178082191780822\n",
            "Batch 2 - Loss: 0.17509657760411892, Accuracy: 0.9315068493150684\n",
            "Batch 3 - Loss: 0.15341716719149137, Accuracy: 0.9285714285714286\n",
            "Batch 1 - Loss: 0.1222614301283352, Accuracy: 0.9315068493150684\n",
            "Batch 2 - Loss: 0.1581964683990316, Accuracy: 0.9041095890410958\n",
            "Batch 3 - Loss: 0.14962128613618894, Accuracy: 0.9285714285714286\n",
            "Batch 1 - Loss: 0.12222957911926402, Accuracy: 0.9315068493150684\n",
            "Batch 2 - Loss: 0.15143542930531143, Accuracy: 0.9041095890410958\n",
            "Batch 3 - Loss: 0.14651182517751332, Accuracy: 0.9285714285714286\n",
            "Batch 1 - Loss: 0.12214715448456959, Accuracy: 0.9315068493150684\n",
            "Batch 2 - Loss: 0.14666065179132762, Accuracy: 0.9041095890410958\n",
            "Batch 3 - Loss: 0.14592585345012937, Accuracy: 0.9285714285714286\n",
            "Batch 1 - Loss: 0.12233804701572236, Accuracy: 0.9315068493150684\n",
            "Batch 2 - Loss: 0.14335992489880453, Accuracy: 0.9041095890410958\n",
            "Batch 3 - Loss: 0.1465337395167727, Accuracy: 0.9285714285714286\n",
            "Batch 1 - Loss: 0.12292637175127613, Accuracy: 0.9315068493150684\n",
            "Batch 2 - Loss: 0.1415801056398509, Accuracy: 0.9041095890410958\n",
            "Batch 3 - Loss: 0.14606478647027027, Accuracy: 0.9285714285714286\n",
            "Batch 1 - Loss: 0.10820747228968212, Accuracy: 0.9393939393939394\n",
            "Batch 1 - Loss: 0.17198459919382553, Accuracy: 0.9393939393939394\n",
            "Batch 2 - Loss: 0.17743315719528013, Accuracy: 0.9090909090909091\n",
            "Batch 2 - Loss: 0.05014859358801364, Accuracy: 1.0\n",
            "Batch 3 - Loss: 0.2503119902581476, Accuracy: 0.9285714285714286\n",
            "Batch 1 - Loss: 0.12704837527669088, Accuracy: 0.9393939393939394\n",
            "Batch 1 - Loss: 0.16088163079759218, Accuracy: 0.9393939393939394\n",
            "Batch 2 - Loss: 0.1708767480180617, Accuracy: 0.9090909090909091\n",
            "Batch 2 - Loss: 0.04973536369162694, Accuracy: 1.0\n",
            "Batch 3 - Loss: 0.2516374869742416, Accuracy: 0.9285714285714286\n",
            "Batch 1 - Loss: 0.1256582163727348, Accuracy: 0.9393939393939394\n",
            "Batch 1 - Loss: 0.15946688283106003, Accuracy: 0.9393939393939394\n",
            "Batch 2 - Loss: 0.16975078465722154, Accuracy: 0.9090909090909091\n",
            "Batch 2 - Loss: 0.05042475479518476, Accuracy: 1.0\n",
            "Batch 3 - Loss: 0.25310904092597364, Accuracy: 0.9285714285714286\n",
            "Batch 1 - Loss: 0.12443791366795134, Accuracy: 0.9393939393939394\n",
            "Batch 1 - Loss: 0.15858752533420756, Accuracy: 0.9393939393939394\n",
            "Batch 2 - Loss: 0.1672102757906783, Accuracy: 0.9090909090909091\n",
            "Batch 2 - Loss: 0.05095082002915406, Accuracy: 1.0\n",
            "Batch 3 - Loss: 0.2513545074170798, Accuracy: 0.9285714285714286\n",
            "Batch 1 - Loss: 0.12351384745983528, Accuracy: 0.9393939393939394\n",
            "Batch 1 - Loss: 0.15773340749716094, Accuracy: 0.9393939393939394\n",
            "Batch 2 - Loss: 0.16687975619110065, Accuracy: 0.9090909090909091\n",
            "Batch 2 - Loss: 0.051220100981608764, Accuracy: 1.0\n",
            "Batch 3 - Loss: 0.25253435164988086, Accuracy: 0.9285714285714286\n",
            "Batch 1 - Loss: 0.12285002878340182, Accuracy: 0.9393939393939394\n",
            "Batch 1 - Loss: 0.15718950918881516, Accuracy: 0.9393939393939394\n",
            "Batch 2 - Loss: 0.16592936886675916, Accuracy: 0.9090909090909091\n",
            "Batch 2 - Loss: 0.05173729518074587, Accuracy: 1.0\n",
            "Batch 3 - Loss: 0.2505376861445212, Accuracy: 0.9285714285714286\n",
            "Batch 1 - Loss: 0.15441209548168097, Accuracy: 0.9347826086956522\n",
            "Batch 1 - Loss: 0.10534341967232737, Accuracy: 0.9347826086956522\n",
            "Batch 2 - Loss: 0.06060305853392967, Accuracy: 0.9782608695652174\n",
            "Batch 3 - Loss: 0.29120812047544214, Accuracy: 0.9090909090909091\n",
            "Batch 1 - Loss: 0.17398676668749796, Accuracy: 0.9347826086956522\n",
            "Batch 1 - Loss: 0.10258600478629548, Accuracy: 0.9565217391304348\n",
            "Batch 2 - Loss: 0.059585849385690934, Accuracy: 0.9782608695652174\n",
            "Batch 3 - Loss: 0.29319778753617814, Accuracy: 0.9090909090909091\n",
            "Batch 1 - Loss: 0.17359421164492503, Accuracy: 0.9347826086956522\n",
            "Batch 1 - Loss: 0.1031638884569446, Accuracy: 0.9565217391304348\n",
            "Batch 2 - Loss: 0.060116738463842655, Accuracy: 0.9782608695652174\n",
            "Batch 3 - Loss: 0.2932290597489395, Accuracy: 0.9090909090909091\n",
            "Batch 1 - Loss: 0.1730468534431174, Accuracy: 0.9347826086956522\n",
            "Batch 1 - Loss: 0.10314472990952664, Accuracy: 0.9565217391304348\n",
            "Batch 2 - Loss: 0.060354949059497766, Accuracy: 0.9782608695652174\n",
            "Batch 3 - Loss: 0.2922790439605214, Accuracy: 0.9090909090909091\n",
            "Batch 1 - Loss: 0.1709697204923862, Accuracy: 0.9347826086956522\n",
            "Batch 1 - Loss: 0.10304086817735748, Accuracy: 0.9565217391304348\n",
            "Batch 2 - Loss: 0.06131718681715613, Accuracy: 0.9782608695652174\n",
            "Batch 3 - Loss: 0.2891777319496068, Accuracy: 0.9090909090909091\n",
            "Batch 1 - Loss: 0.1708479796688604, Accuracy: 0.9347826086956522\n",
            "Batch 1 - Loss: 0.10278387787945067, Accuracy: 0.9565217391304348\n",
            "Batch 2 - Loss: 0.06095326602728369, Accuracy: 0.9782608695652174\n",
            "Batch 3 - Loss: 0.2925775790371468, Accuracy: 0.9090909090909091\n",
            "Batch 1 - Loss: 0.15505087551811042, Accuracy: 0.9393939393939394\n",
            "Batch 2 - Loss: 0.12875521605906953, Accuracy: 0.9242424242424242\n",
            "Batch 3 - Loss: 0.20946805773336255, Accuracy: 0.9285714285714286\n",
            "Batch 1 - Loss: 0.1248392870595592, Accuracy: 0.9393939393939394\n",
            "Batch 2 - Loss: 0.12345604849036629, Accuracy: 0.9242424242424242\n",
            "Batch 3 - Loss: 0.21244538480946032, Accuracy: 0.9285714285714286\n",
            "Batch 1 - Loss: 0.1259677229606087, Accuracy: 0.9393939393939394\n",
            "Batch 2 - Loss: 0.12335472453399265, Accuracy: 0.9242424242424242\n",
            "Batch 3 - Loss: 0.2126605499121559, Accuracy: 0.9285714285714286\n",
            "Batch 1 - Loss: 0.12618279939088073, Accuracy: 0.9393939393939394\n",
            "Batch 2 - Loss: 0.12273817205933703, Accuracy: 0.9242424242424242\n",
            "Batch 3 - Loss: 0.21310525441740294, Accuracy: 0.9285714285714286\n",
            "Batch 1 - Loss: 0.12623436074513572, Accuracy: 0.9393939393939394\n",
            "Batch 2 - Loss: 0.12248424844975032, Accuracy: 0.9242424242424242\n",
            "Batch 3 - Loss: 0.21320518516610537, Accuracy: 0.9285714285714286\n",
            "Batch 1 - Loss: 0.12553150587976836, Accuracy: 0.9393939393939394\n",
            "Batch 2 - Loss: 0.12159408663690353, Accuracy: 0.9242424242424242\n",
            "Batch 3 - Loss: 0.21309965588630939, Accuracy: 0.9285714285714286\n",
            "Batch 1 - Loss: 0.14465739586845547, Accuracy: 0.9342105263157895\n",
            "Batch 2 - Loss: 0.11009930780855189, Accuracy: 0.9342105263157895\n",
            "Batch 3 - Loss: 0.20267887532558865, Accuracy: 0.875\n",
            "Batch 1 - Loss: 0.23037994369044926, Accuracy: 0.9342105263157895\n",
            "Batch 2 - Loss: 0.10599144628411941, Accuracy: 0.9605263157894737\n",
            "Batch 3 - Loss: 0.2051734990117342, Accuracy: 0.875\n",
            "Batch 1 - Loss: 0.21849154995756093, Accuracy: 0.9342105263157895\n",
            "Batch 2 - Loss: 0.1411288194553948, Accuracy: 0.9342105263157895\n",
            "Batch 3 - Loss: 0.17483932049425532, Accuracy: 0.875\n",
            "Batch 1 - Loss: 0.19910865081398843, Accuracy: 0.9342105263157895\n",
            "Batch 2 - Loss: 0.14803549724346673, Accuracy: 0.9342105263157895\n",
            "Batch 3 - Loss: 0.1655604980642275, Accuracy: 0.875\n",
            "Batch 1 - Loss: 0.19602963551516667, Accuracy: 0.9342105263157895\n",
            "Batch 2 - Loss: 0.14710526998016626, Accuracy: 0.9342105263157895\n",
            "Batch 3 - Loss: 0.16392472291035534, Accuracy: 0.875\n",
            "Batch 1 - Loss: 0.1941722071264518, Accuracy: 0.9342105263157895\n",
            "Batch 2 - Loss: 0.14603792428031445, Accuracy: 0.9342105263157895\n",
            "Batch 3 - Loss: 0.16235618837148036, Accuracy: 0.875\n",
            "Batch 1 - Loss: 0.1603532366099568, Accuracy: 0.9428571428571428\n",
            "Batch 1 - Loss: 0.13666277756144343, Accuracy: 0.9142857142857143\n",
            "Batch 2 - Loss: 0.21772409905602688, Accuracy: 0.8857142857142857\n",
            "Batch 2 - Loss: 0.03887089861786885, Accuracy: 1.0\n",
            "Batch 3 - Loss: 0.3341299486219767, Accuracy: 0.9\n",
            "Batch 1 - Loss: 0.2175534045068751, Accuracy: 0.9428571428571428\n",
            "Batch 1 - Loss: 0.15069155333505888, Accuracy: 0.9428571428571428\n",
            "Batch 2 - Loss: 0.22267987248726828, Accuracy: 0.8857142857142857\n",
            "Batch 2 - Loss: 0.03849279222404512, Accuracy: 1.0\n",
            "Batch 3 - Loss: 0.337610113268668, Accuracy: 0.9\n",
            "Batch 1 - Loss: 0.2040924760155157, Accuracy: 0.9428571428571428\n",
            "Batch 1 - Loss: 0.13987108513323437, Accuracy: 0.9428571428571428\n",
            "Batch 2 - Loss: 0.2181918326654255, Accuracy: 0.8857142857142857\n",
            "Batch 2 - Loss: 0.039416038540630145, Accuracy: 1.0\n",
            "Batch 3 - Loss: 0.33285922363152665, Accuracy: 0.9\n",
            "Batch 1 - Loss: 0.19977960124998148, Accuracy: 0.9428571428571428\n",
            "Batch 1 - Loss: 0.13830085750246227, Accuracy: 0.9428571428571428\n",
            "Batch 2 - Loss: 0.2157784965264735, Accuracy: 0.8857142857142857\n",
            "Batch 2 - Loss: 0.0401024427763143, Accuracy: 1.0\n",
            "Batch 3 - Loss: 0.32960869927091696, Accuracy: 0.9\n",
            "Batch 1 - Loss: 0.19405266790412215, Accuracy: 0.9428571428571428\n",
            "Batch 1 - Loss: 0.1369460703461465, Accuracy: 0.9428571428571428\n",
            "Batch 2 - Loss: 0.21272550697381934, Accuracy: 0.8857142857142857\n",
            "Batch 2 - Loss: 0.04122746438450402, Accuracy: 1.0\n",
            "Batch 3 - Loss: 0.3261556650422193, Accuracy: 0.9\n",
            "Batch 1 - Loss: 0.1932534570781018, Accuracy: 0.9428571428571428\n",
            "Batch 1 - Loss: 0.1357411957866798, Accuracy: 0.9428571428571428\n",
            "Batch 2 - Loss: 0.21248356408665967, Accuracy: 0.8857142857142857\n",
            "Batch 2 - Loss: 0.04142449621805447, Accuracy: 1.0\n",
            "Batch 3 - Loss: 0.3250361009193735, Accuracy: 0.9\n",
            "Batch 1 - Loss: 0.22410456021373115, Accuracy: 0.9302325581395349\n",
            "Batch 2 - Loss: 0.1284411755781992, Accuracy: 0.9324324324324325\n",
            "Batch 1 - Loss: 0.1129395048910687, Accuracy: 0.9534883720930233\n",
            "Batch 2 - Loss: 0.10251334544389244, Accuracy: 0.9459459459459459\n",
            "Batch 1 - Loss: 0.11281589950607686, Accuracy: 0.9534883720930233\n",
            "Batch 2 - Loss: 0.10058751719270474, Accuracy: 0.9459459459459459\n",
            "Batch 1 - Loss: 0.11226393765590062, Accuracy: 0.9534883720930233\n",
            "Batch 2 - Loss: 0.09938743427090795, Accuracy: 0.9459459459459459\n",
            "Batch 1 - Loss: 0.11177973156460627, Accuracy: 0.9534883720930233\n",
            "Batch 2 - Loss: 0.09831169849941682, Accuracy: 0.9459459459459459\n",
            "Batch 1 - Loss: 0.11126660112029468, Accuracy: 0.9534883720930233\n",
            "Batch 2 - Loss: 0.09744649714573039, Accuracy: 0.9459459459459459\n",
            "Batch 1 - Loss: 0.10956717373659838, Accuracy: 0.9540229885057471\n",
            "Batch 2 - Loss: 0.09795008951022229, Accuracy: 0.9452054794520548\n",
            "Batch 1 - Loss: 0.10919605646968097, Accuracy: 0.9540229885057471\n",
            "Batch 2 - Loss: 0.09720909819497306, Accuracy: 0.9452054794520548\n",
            "Batch 1 - Loss: 0.10879365715501664, Accuracy: 0.9540229885057471\n",
            "Batch 2 - Loss: 0.09665879832584477, Accuracy: 0.9452054794520548\n",
            "Batch 1 - Loss: 0.1082951944508863, Accuracy: 0.9540229885057471\n",
            "Batch 2 - Loss: 0.09628540884887901, Accuracy: 0.9452054794520548\n",
            "Batch 1 - Loss: 0.1079306045722226, Accuracy: 0.9540229885057471\n",
            "Batch 2 - Loss: 0.09589193809642743, Accuracy: 0.9452054794520548\n",
            "Batch 1 - Loss: 0.10760218703542213, Accuracy: 0.9540229885057471\n",
            "Batch 2 - Loss: 0.09555321768933493, Accuracy: 0.9452054794520548\n",
            "Batch 1 - Loss: 0.11384776464427024, Accuracy: 0.9534883720930233\n",
            "Batch 1 - Loss: 0.13158981870179365, Accuracy: 0.9302325581395349\n",
            "Batch 2 - Loss: 0.0449801548829707, Accuracy: 1.0\n",
            "Batch 3 - Loss: 0.23555243878223542, Accuracy: 0.9354838709677419\n",
            "Batch 1 - Loss: 0.11416204155189548, Accuracy: 0.9534883720930233\n",
            "Batch 1 - Loss: 0.14645304943595983, Accuracy: 0.9302325581395349\n",
            "Batch 2 - Loss: 0.05023797664042659, Accuracy: 0.9767441860465116\n",
            "Batch 3 - Loss: 0.23537463020326163, Accuracy: 0.9354838709677419\n",
            "Batch 1 - Loss: 0.11095437243319828, Accuracy: 0.9534883720930233\n",
            "Batch 1 - Loss: 0.1491701910838924, Accuracy: 0.9302325581395349\n",
            "Batch 2 - Loss: 0.05322995500767672, Accuracy: 0.9767441860465116\n",
            "Batch 3 - Loss: 0.2369325897878124, Accuracy: 0.9354838709677419\n",
            "Batch 1 - Loss: 0.10961637975547203, Accuracy: 0.9534883720930233\n",
            "Batch 1 - Loss: 0.14919127007605731, Accuracy: 0.9302325581395349\n",
            "Batch 2 - Loss: 0.05523382764932215, Accuracy: 0.9767441860465116\n",
            "Batch 3 - Loss: 0.23810618456007865, Accuracy: 0.9354838709677419\n",
            "Batch 1 - Loss: 0.10904896696893694, Accuracy: 0.9534883720930233\n",
            "Batch 1 - Loss: 0.14877245155551616, Accuracy: 0.9302325581395349\n",
            "Batch 2 - Loss: 0.05610019188148243, Accuracy: 0.9767441860465116\n",
            "Batch 3 - Loss: 0.2374284889610391, Accuracy: 0.9354838709677419\n",
            "Batch 1 - Loss: 0.10852057237377126, Accuracy: 0.9534883720930233\n",
            "Batch 1 - Loss: 0.1481034550978511, Accuracy: 0.9302325581395349\n",
            "Batch 2 - Loss: 0.056688191785262, Accuracy: 0.9767441860465116\n",
            "Batch 3 - Loss: 0.23646531591427983, Accuracy: 0.9354838709677419\n",
            "Batch 1 - Loss: 0.13972147715227037, Accuracy: 0.9361702127659575\n",
            "Batch 1 - Loss: 0.09770094361432334, Accuracy: 0.9361702127659575\n",
            "Batch 2 - Loss: 0.08216769217575609, Accuracy: 0.9574468085106383\n",
            "Batch 3 - Loss: 0.3974240052214045, Accuracy: 0.8947368421052632\n",
            "Batch 1 - Loss: 0.29666497143425435, Accuracy: 0.8723404255319149\n",
            "Batch 1 - Loss: 0.03852238735780455, Accuracy: 0.9787234042553191\n",
            "Batch 2 - Loss: 0.04988549662734095, Accuracy: 0.9787234042553191\n",
            "Batch 3 - Loss: 0.5563106428370921, Accuracy: 0.8947368421052632\n",
            "Batch 1 - Loss: 0.22068726446419668, Accuracy: 0.9148936170212766\n",
            "Batch 1 - Loss: 0.16088975618621784, Accuracy: 0.9361702127659575\n",
            "Batch 2 - Loss: 0.17455766381668242, Accuracy: 0.9361702127659575\n",
            "Batch 3 - Loss: 0.3361589430730425, Accuracy: 0.8947368421052632\n",
            "Batch 1 - Loss: 0.20854621375874668, Accuracy: 0.9361702127659575\n",
            "Batch 1 - Loss: 0.11469669221860342, Accuracy: 0.9361702127659575\n",
            "Batch 2 - Loss: 0.11732321529742894, Accuracy: 0.9574468085106383\n",
            "Batch 3 - Loss: 0.37511783624731554, Accuracy: 0.8947368421052632\n",
            "Batch 1 - Loss: 0.21786775572356962, Accuracy: 0.9361702127659575\n",
            "Batch 1 - Loss: 0.09900647756820437, Accuracy: 0.9361702127659575\n",
            "Batch 2 - Loss: 0.1012605995227025, Accuracy: 0.9574468085106383\n",
            "Batch 3 - Loss: 0.39190879358650466, Accuracy: 0.8947368421052632\n",
            "Batch 1 - Loss: 0.21739182048088307, Accuracy: 0.9361702127659575\n",
            "Batch 1 - Loss: 0.0993049977811329, Accuracy: 0.9361702127659575\n",
            "Batch 2 - Loss: 0.10241763270398001, Accuracy: 0.9574468085106383\n",
            "Batch 3 - Loss: 0.3885908145677933, Accuracy: 0.8947368421052632\n",
            "Batch 1 - Loss: 0.20745810069161286, Accuracy: 0.9452054794520548\n",
            "Batch 2 - Loss: 0.16407032539087665, Accuracy: 0.9178082191780822\n",
            "Batch 3 - Loss: 0.13158111040620482, Accuracy: 0.9285714285714286\n",
            "Batch 1 - Loss: 0.10653190605297058, Accuracy: 0.9315068493150684\n",
            "Batch 2 - Loss: 0.15578320571229012, Accuracy: 0.9178082191780822\n",
            "Batch 3 - Loss: 0.13043402321948505, Accuracy: 0.9285714285714286\n",
            "Batch 1 - Loss: 0.10596156579286681, Accuracy: 0.9315068493150684\n",
            "Batch 2 - Loss: 0.14888334505520798, Accuracy: 0.9041095890410958\n",
            "Batch 3 - Loss: 0.13099253753346432, Accuracy: 0.9285714285714286\n",
            "Batch 1 - Loss: 0.10729525535584061, Accuracy: 0.9315068493150684\n",
            "Batch 2 - Loss: 0.14660611715743255, Accuracy: 0.9041095890410958\n",
            "Batch 3 - Loss: 0.13088514491138406, Accuracy: 0.9285714285714286\n",
            "Batch 1 - Loss: 0.1066109283496704, Accuracy: 0.9315068493150684\n",
            "Batch 2 - Loss: 0.142913506507532, Accuracy: 0.9041095890410958\n",
            "Batch 3 - Loss: 0.13218754828230692, Accuracy: 0.9285714285714286\n",
            "Batch 1 - Loss: 0.10795940293684748, Accuracy: 0.9315068493150684\n",
            "Batch 2 - Loss: 0.1423030481008191, Accuracy: 0.9041095890410958\n",
            "Batch 3 - Loss: 0.13224699760686698, Accuracy: 0.9285714285714286\n",
            "Batch 1 - Loss: 0.10728650946197661, Accuracy: 0.9315068493150684\n",
            "Batch 2 - Loss: 0.1399666303018713, Accuracy: 0.9041095890410958\n",
            "Batch 3 - Loss: 0.13300163205645232, Accuracy: 0.9285714285714286\n",
            "Batch 1 - Loss: 0.10442384459387351, Accuracy: 0.9393939393939394\n",
            "Batch 1 - Loss: 0.14690382938436602, Accuracy: 0.9393939393939394\n",
            "Batch 2 - Loss: 0.16409917079916142, Accuracy: 0.9090909090909091\n",
            "Batch 2 - Loss: 0.0513189580892096, Accuracy: 0.9696969696969697\n",
            "Batch 3 - Loss: 0.2524272664907449, Accuracy: 0.9285714285714286\n",
            "Batch 1 - Loss: 0.11709520501614362, Accuracy: 0.9393939393939394\n",
            "Batch 1 - Loss: 0.1396665402567772, Accuracy: 0.9393939393939394\n",
            "Batch 2 - Loss: 0.15986222316805163, Accuracy: 0.9090909090909091\n",
            "Batch 2 - Loss: 0.05096730955934254, Accuracy: 0.9696969696969697\n",
            "Batch 3 - Loss: 0.2497948735173193, Accuracy: 0.9285714285714286\n",
            "Batch 1 - Loss: 0.11647593484486696, Accuracy: 0.9393939393939394\n",
            "Batch 1 - Loss: 0.1388588717560437, Accuracy: 0.9393939393939394\n",
            "Batch 2 - Loss: 0.16035742111810755, Accuracy: 0.9090909090909091\n",
            "Batch 2 - Loss: 0.05172494289416461, Accuracy: 0.9696969696969697\n",
            "Batch 3 - Loss: 0.2489532637774308, Accuracy: 0.9285714285714286\n",
            "Batch 1 - Loss: 0.11598821965656014, Accuracy: 0.9393939393939394\n",
            "Batch 1 - Loss: 0.13805329329239016, Accuracy: 0.9393939393939394\n",
            "Batch 2 - Loss: 0.16014473056517756, Accuracy: 0.9090909090909091\n",
            "Batch 2 - Loss: 0.052387291236181924, Accuracy: 0.9696969696969697\n",
            "Batch 3 - Loss: 0.24828903236000158, Accuracy: 0.9285714285714286\n",
            "Batch 1 - Loss: 0.1155685682101083, Accuracy: 0.9393939393939394\n",
            "Batch 1 - Loss: 0.13731137151459943, Accuracy: 0.9393939393939394\n",
            "Batch 2 - Loss: 0.15997863442529536, Accuracy: 0.9090909090909091\n",
            "Batch 2 - Loss: 0.052742398523270805, Accuracy: 0.9696969696969697\n",
            "Batch 3 - Loss: 0.24785034433932837, Accuracy: 0.9285714285714286\n",
            "Batch 1 - Loss: 0.11525541398213485, Accuracy: 0.9393939393939394\n",
            "Batch 1 - Loss: 0.13666771993270582, Accuracy: 0.9393939393939394\n",
            "Batch 2 - Loss: 0.15984478581779266, Accuracy: 0.9090909090909091\n",
            "Batch 2 - Loss: 0.05303026929139507, Accuracy: 0.9696969696969697\n",
            "Batch 3 - Loss: 0.24831607517708046, Accuracy: 0.9285714285714286\n",
            "Batch 1 - Loss: 0.115075873621975, Accuracy: 0.9393939393939394\n",
            "Batch 1 - Loss: 0.1361660671075021, Accuracy: 0.9393939393939394\n",
            "Batch 2 - Loss: 0.15958309170419574, Accuracy: 0.9090909090909091\n",
            "Batch 2 - Loss: 0.053334542424377684, Accuracy: 0.9696969696969697\n",
            "Batch 3 - Loss: 0.24704129443852865, Accuracy: 0.9285714285714286\n",
            "Batch 1 - Loss: 0.13675758960338102, Accuracy: 0.9347826086956522\n",
            "Batch 1 - Loss: 0.09808103404720371, Accuracy: 0.9565217391304348\n",
            "Batch 2 - Loss: 0.060707729472903774, Accuracy: 0.9782608695652174\n",
            "Batch 3 - Loss: 0.2824134980341521, Accuracy: 0.9090909090909091\n",
            "Batch 1 - Loss: 0.15569775869532312, Accuracy: 0.9347826086956522\n",
            "Batch 1 - Loss: 0.09616545575978427, Accuracy: 0.9565217391304348\n",
            "Batch 2 - Loss: 0.05986250605137881, Accuracy: 0.9782608695652174\n",
            "Batch 3 - Loss: 0.28441730827838596, Accuracy: 0.9090909090909091\n",
            "Batch 1 - Loss: 0.15518260737097547, Accuracy: 0.9347826086956522\n",
            "Batch 1 - Loss: 0.09728524853746863, Accuracy: 0.9565217391304348\n",
            "Batch 2 - Loss: 0.06054737982232737, Accuracy: 0.9782608695652174\n",
            "Batch 3 - Loss: 0.28474400996903637, Accuracy: 0.9090909090909091\n",
            "Batch 1 - Loss: 0.1546400213286295, Accuracy: 0.9347826086956522\n",
            "Batch 1 - Loss: 0.09749777113554191, Accuracy: 0.9565217391304348\n",
            "Batch 2 - Loss: 0.0611989701977202, Accuracy: 0.9782608695652174\n",
            "Batch 3 - Loss: 0.2846274258764465, Accuracy: 0.9090909090909091\n",
            "Batch 1 - Loss: 0.15426076275734021, Accuracy: 0.9347826086956522\n",
            "Batch 1 - Loss: 0.09764671501321655, Accuracy: 0.9565217391304348\n",
            "Batch 2 - Loss: 0.06121296082309311, Accuracy: 0.9782608695652174\n",
            "Batch 3 - Loss: 0.28528270984790177, Accuracy: 0.9090909090909091\n",
            "Batch 1 - Loss: 0.15414793077304426, Accuracy: 0.9347826086956522\n",
            "Batch 1 - Loss: 0.09729815909450949, Accuracy: 0.9565217391304348\n",
            "Batch 2 - Loss: 0.061428227845550215, Accuracy: 0.9782608695652174\n",
            "Batch 3 - Loss: 0.28557322017965703, Accuracy: 0.9090909090909091\n",
            "Batch 1 - Loss: 0.15399231790888188, Accuracy: 0.9347826086956522\n",
            "Batch 1 - Loss: 0.09700844046040816, Accuracy: 0.9565217391304348\n",
            "Batch 2 - Loss: 0.06156955610316083, Accuracy: 0.9782608695652174\n",
            "Batch 3 - Loss: 0.28569327332644906, Accuracy: 0.9090909090909091\n",
            "Batch 1 - Loss: 0.14017660690685255, Accuracy: 0.9393939393939394\n",
            "Batch 2 - Loss: 0.12377088434323011, Accuracy: 0.9242424242424242\n",
            "Batch 3 - Loss: 0.20282024564883325, Accuracy: 0.9285714285714286\n",
            "Batch 1 - Loss: 0.11128976244077877, Accuracy: 0.9393939393939394\n",
            "Batch 2 - Loss: 0.1186914698336228, Accuracy: 0.9242424242424242\n",
            "Batch 3 - Loss: 0.20616281727311328, Accuracy: 0.9285714285714286\n",
            "Batch 1 - Loss: 0.11226802814917236, Accuracy: 0.9393939393939394\n",
            "Batch 2 - Loss: 0.11861808509197513, Accuracy: 0.9242424242424242\n",
            "Batch 3 - Loss: 0.20661063998647794, Accuracy: 0.9285714285714286\n",
            "Batch 1 - Loss: 0.111828841215718, Accuracy: 0.9393939393939394\n",
            "Batch 2 - Loss: 0.11757473031476938, Accuracy: 0.9242424242424242\n",
            "Batch 3 - Loss: 0.20717599461244232, Accuracy: 0.9285714285714286\n",
            "Batch 1 - Loss: 0.11252147103706678, Accuracy: 0.9393939393939394\n",
            "Batch 2 - Loss: 0.11763412863127147, Accuracy: 0.9242424242424242\n",
            "Batch 3 - Loss: 0.20736725610319745, Accuracy: 0.9285714285714286\n",
            "Batch 1 - Loss: 0.1126090070297226, Accuracy: 0.9393939393939394\n",
            "Batch 2 - Loss: 0.11744747778991266, Accuracy: 0.9242424242424242\n",
            "Batch 3 - Loss: 0.2075943534567657, Accuracy: 0.9285714285714286\n",
            "Batch 1 - Loss: 0.1126725941858, Accuracy: 0.9393939393939394\n",
            "Batch 2 - Loss: 0.1173153456384747, Accuracy: 0.9242424242424242\n",
            "Batch 3 - Loss: 0.20772910435079675, Accuracy: 0.9285714285714286\n",
            "Batch 1 - Loss: 0.1312417910628405, Accuracy: 0.9342105263157895\n",
            "Batch 2 - Loss: 0.10761091963502759, Accuracy: 0.9342105263157895\n",
            "Batch 3 - Loss: 0.1943891490612746, Accuracy: 0.875\n",
            "Batch 1 - Loss: 0.21839892322269228, Accuracy: 0.9342105263157895\n",
            "Batch 2 - Loss: 0.10143404390688067, Accuracy: 0.9605263157894737\n",
            "Batch 3 - Loss: 0.19811566072798983, Accuracy: 0.875\n",
            "Batch 1 - Loss: 0.20193084018287233, Accuracy: 0.9342105263157895\n",
            "Batch 2 - Loss: 0.13712512983634464, Accuracy: 0.9342105263157895\n",
            "Batch 3 - Loss: 0.16420912990048575, Accuracy: 0.875\n",
            "Batch 1 - Loss: 0.1813510552084727, Accuracy: 0.9342105263157895\n",
            "Batch 2 - Loss: 0.14645203623314576, Accuracy: 0.9342105263157895\n",
            "Batch 3 - Loss: 0.15361823743488087, Accuracy: 0.875\n",
            "Batch 1 - Loss: 0.17606568813647266, Accuracy: 0.9342105263157895\n",
            "Batch 2 - Loss: 0.14570389621543398, Accuracy: 0.9342105263157895\n",
            "Batch 3 - Loss: 0.15229820094902322, Accuracy: 0.875\n",
            "Batch 1 - Loss: 0.17595528190838894, Accuracy: 0.9342105263157895\n",
            "Batch 2 - Loss: 0.1452355881130108, Accuracy: 0.9342105263157895\n",
            "Batch 3 - Loss: 0.1511235357605278, Accuracy: 0.875\n",
            "Batch 1 - Loss: 0.17613452270373764, Accuracy: 0.9342105263157895\n",
            "Batch 2 - Loss: 0.14539456994812125, Accuracy: 0.9342105263157895\n",
            "Batch 3 - Loss: 0.14944844676127314, Accuracy: 0.875\n",
            "Batch 1 - Loss: 0.15157667311376513, Accuracy: 0.9428571428571428\n",
            "Batch 1 - Loss: 0.11331198176097632, Accuracy: 0.9142857142857143\n",
            "Batch 2 - Loss: 0.22040746335257838, Accuracy: 0.8857142857142857\n",
            "Batch 2 - Loss: 0.03776885667596157, Accuracy: 1.0\n",
            "Batch 3 - Loss: 0.3454708835267965, Accuracy: 0.9\n",
            "Batch 1 - Loss: 0.21812572119263937, Accuracy: 0.9428571428571428\n",
            "Batch 1 - Loss: 0.13309601108567087, Accuracy: 0.9428571428571428\n",
            "Batch 2 - Loss: 0.22194178678050067, Accuracy: 0.8857142857142857\n",
            "Batch 2 - Loss: 0.03727980883635429, Accuracy: 1.0\n",
            "Batch 3 - Loss: 0.34988299822232766, Accuracy: 0.9\n",
            "Batch 1 - Loss: 0.2033566572461878, Accuracy: 0.9428571428571428\n",
            "Batch 1 - Loss: 0.12004043015469537, Accuracy: 0.9428571428571428\n",
            "Batch 2 - Loss: 0.2179175699067648, Accuracy: 0.8857142857142857\n",
            "Batch 2 - Loss: 0.038195702587325644, Accuracy: 1.0\n",
            "Batch 3 - Loss: 0.3444204885197134, Accuracy: 0.9\n",
            "Batch 1 - Loss: 0.20030896851405613, Accuracy: 0.9428571428571428\n",
            "Batch 1 - Loss: 0.11917555725346371, Accuracy: 0.9428571428571428\n",
            "Batch 2 - Loss: 0.21536207729068854, Accuracy: 0.8857142857142857\n",
            "Batch 2 - Loss: 0.03872579947403813, Accuracy: 1.0\n",
            "Batch 3 - Loss: 0.34106583325906475, Accuracy: 0.9\n",
            "Batch 1 - Loss: 0.19772101332713274, Accuracy: 0.9428571428571428\n",
            "Batch 1 - Loss: 0.1179744927621846, Accuracy: 0.9428571428571428\n",
            "Batch 2 - Loss: 0.2139078968443302, Accuracy: 0.8857142857142857\n",
            "Batch 2 - Loss: 0.039126095599953004, Accuracy: 1.0\n",
            "Batch 3 - Loss: 0.33565386528324154, Accuracy: 0.9\n",
            "Batch 1 - Loss: 0.19436561628163193, Accuracy: 0.9428571428571428\n",
            "Batch 1 - Loss: 0.11613845969636262, Accuracy: 0.9428571428571428\n",
            "Batch 2 - Loss: 0.21336880534162395, Accuracy: 0.8857142857142857\n",
            "Batch 2 - Loss: 0.039538338424043716, Accuracy: 1.0\n",
            "Batch 3 - Loss: 0.33496587872483236, Accuracy: 0.9\n",
            "Batch 1 - Loss: 0.19289585271850235, Accuracy: 0.9428571428571428\n",
            "Batch 1 - Loss: 0.11540241331885358, Accuracy: 0.9428571428571428\n",
            "Batch 2 - Loss: 0.21263712342292743, Accuracy: 0.8857142857142857\n",
            "Batch 2 - Loss: 0.03995052427793412, Accuracy: 1.0\n",
            "Batch 3 - Loss: 0.3288704864760668, Accuracy: 0.9\n",
            "Batch 1 - Loss: 0.21451680033507578, Accuracy: 0.9302325581395349\n",
            "Batch 2 - Loss: 0.12647009292412922, Accuracy: 0.9324324324324325\n",
            "Batch 1 - Loss: 0.10341268810729808, Accuracy: 0.9534883720930233\n",
            "Batch 2 - Loss: 0.10301665849724521, Accuracy: 0.9459459459459459\n",
            "Batch 1 - Loss: 0.10339267059463605, Accuracy: 0.9534883720930233\n",
            "Batch 2 - Loss: 0.1006155225085034, Accuracy: 0.9459459459459459\n",
            "Batch 1 - Loss: 0.10311997033362008, Accuracy: 0.9534883720930233\n",
            "Batch 2 - Loss: 0.09918082436111236, Accuracy: 0.9459459459459459\n",
            "Batch 1 - Loss: 0.10278160364848397, Accuracy: 0.9534883720930233\n",
            "Batch 2 - Loss: 0.09797269708106444, Accuracy: 0.9459459459459459\n",
            "Batch 1 - Loss: 0.10246617483434138, Accuracy: 0.9534883720930233\n",
            "Batch 2 - Loss: 0.0969510502317775, Accuracy: 0.9459459459459459\n",
            "Batch 1 - Loss: 0.10218302354868937, Accuracy: 0.9534883720930233\n",
            "Batch 2 - Loss: 0.09603076442416295, Accuracy: 0.9459459459459459\n",
            "Batch 1 - Loss: 0.10073980910978649, Accuracy: 0.9540229885057471\n",
            "Batch 2 - Loss: 0.09659337554913633, Accuracy: 0.9452054794520548\n",
            "Batch 1 - Loss: 0.10051372570542828, Accuracy: 0.9540229885057471\n",
            "Batch 2 - Loss: 0.09603064047744164, Accuracy: 0.9452054794520548\n",
            "Batch 1 - Loss: 0.10021900154401223, Accuracy: 0.9540229885057471\n",
            "Batch 2 - Loss: 0.09548936078062448, Accuracy: 0.9452054794520548\n",
            "Batch 1 - Loss: 0.0999562493618819, Accuracy: 0.9540229885057471\n",
            "Batch 2 - Loss: 0.09508047728947938, Accuracy: 0.9452054794520548\n",
            "Batch 1 - Loss: 0.09973419108275991, Accuracy: 0.9540229885057471\n",
            "Batch 2 - Loss: 0.09470066980604017, Accuracy: 0.9452054794520548\n",
            "Batch 1 - Loss: 0.09955078411391098, Accuracy: 0.9540229885057471\n",
            "Batch 2 - Loss: 0.09433055922796406, Accuracy: 0.9452054794520548\n",
            "Batch 1 - Loss: 0.09933595367533125, Accuracy: 0.9540229885057471\n",
            "Batch 2 - Loss: 0.09401876863234283, Accuracy: 0.9452054794520548\n",
            "Batch 1 - Loss: 0.11288739772355276, Accuracy: 0.9534883720930233\n",
            "Batch 1 - Loss: 0.11605962847612515, Accuracy: 0.9302325581395349\n",
            "Batch 2 - Loss: 0.04381964369502904, Accuracy: 1.0\n",
            "Batch 3 - Loss: 0.2331023376638319, Accuracy: 0.9354838709677419\n",
            "Batch 1 - Loss: 0.11276849486095811, Accuracy: 0.9534883720930233\n",
            "Batch 1 - Loss: 0.12982694322907448, Accuracy: 0.9302325581395349\n",
            "Batch 2 - Loss: 0.04940818801869172, Accuracy: 0.9767441860465116\n",
            "Batch 3 - Loss: 0.23132219924149794, Accuracy: 0.9354838709677419\n",
            "Batch 1 - Loss: 0.10920127650299015, Accuracy: 0.9534883720930233\n",
            "Batch 1 - Loss: 0.13297993814524223, Accuracy: 0.9302325581395349\n",
            "Batch 2 - Loss: 0.05206537297511083, Accuracy: 0.9767441860465116\n",
            "Batch 3 - Loss: 0.2330231273685111, Accuracy: 0.9354838709677419\n",
            "Batch 1 - Loss: 0.10791230293120668, Accuracy: 0.9534883720930233\n",
            "Batch 1 - Loss: 0.13356359124086054, Accuracy: 0.9302325581395349\n",
            "Batch 2 - Loss: 0.053820366311986896, Accuracy: 0.9767441860465116\n",
            "Batch 3 - Loss: 0.23283334422734442, Accuracy: 0.9354838709677419\n",
            "Batch 1 - Loss: 0.10700856584632067, Accuracy: 0.9534883720930233\n",
            "Batch 1 - Loss: 0.13347891214690072, Accuracy: 0.9302325581395349\n",
            "Batch 2 - Loss: 0.05487051548762703, Accuracy: 0.9767441860465116\n",
            "Batch 3 - Loss: 0.23132324125964432, Accuracy: 0.9354838709677419\n",
            "Batch 1 - Loss: 0.10648188435725285, Accuracy: 0.9534883720930233\n",
            "Batch 1 - Loss: 0.13320878409594838, Accuracy: 0.9302325581395349\n",
            "Batch 2 - Loss: 0.055330536077498764, Accuracy: 0.9767441860465116\n",
            "Batch 3 - Loss: 0.23359086245449615, Accuracy: 0.9354838709677419\n",
            "Batch 1 - Loss: 0.10638810916086944, Accuracy: 0.9534883720930233\n",
            "Batch 1 - Loss: 0.1329873208092003, Accuracy: 0.9302325581395349\n",
            "Batch 2 - Loss: 0.05580235102670072, Accuracy: 0.9767441860465116\n",
            "Batch 3 - Loss: 0.23233570637553871, Accuracy: 0.9354838709677419\n",
            "Batch 1 - Loss: 0.12689522513376864, Accuracy: 0.9361702127659575\n",
            "Batch 1 - Loss: 0.09441227650129717, Accuracy: 0.9574468085106383\n",
            "Batch 2 - Loss: 0.08166880777727112, Accuracy: 0.9574468085106383\n",
            "Batch 3 - Loss: 0.38893272894695285, Accuracy: 0.8947368421052632\n",
            "Batch 1 - Loss: 0.2851292174001786, Accuracy: 0.8723404255319149\n",
            "Batch 1 - Loss: 0.03714951325874515, Accuracy: 0.9787234042553191\n",
            "Batch 2 - Loss: 0.04938121663605953, Accuracy: 0.9787234042553191\n",
            "Batch 3 - Loss: 0.5424957062936433, Accuracy: 0.8947368421052632\n",
            "Batch 1 - Loss: 0.21288304999169266, Accuracy: 0.9148936170212766\n",
            "Batch 1 - Loss: 0.14710534648182055, Accuracy: 0.9361702127659575\n",
            "Batch 2 - Loss: 0.16376447027364896, Accuracy: 0.9361702127659575\n",
            "Batch 3 - Loss: 0.340682423649924, Accuracy: 0.8947368421052632\n",
            "Batch 1 - Loss: 0.1999413133539177, Accuracy: 0.9361702127659575\n",
            "Batch 1 - Loss: 0.11107807839005272, Accuracy: 0.9361702127659575\n",
            "Batch 2 - Loss: 0.11850620199593584, Accuracy: 0.9574468085106383\n",
            "Batch 3 - Loss: 0.37305172087314603, Accuracy: 0.8947368421052632\n",
            "Batch 1 - Loss: 0.20789142342271372, Accuracy: 0.9361702127659575\n",
            "Batch 1 - Loss: 0.09695961923517647, Accuracy: 0.9361702127659575\n",
            "Batch 2 - Loss: 0.1042657290499783, Accuracy: 0.9574468085106383\n",
            "Batch 3 - Loss: 0.389106355997548, Accuracy: 0.8947368421052632\n",
            "Batch 1 - Loss: 0.20729130559857667, Accuracy: 0.9361702127659575\n",
            "Batch 1 - Loss: 0.09673487398069441, Accuracy: 0.9361702127659575\n",
            "Batch 2 - Loss: 0.10505151433619504, Accuracy: 0.9574468085106383\n",
            "Batch 3 - Loss: 0.3856408855469476, Accuracy: 0.8947368421052632\n",
            "Batch 1 - Loss: 0.20418707229991598, Accuracy: 0.9361702127659575\n",
            "Batch 1 - Loss: 0.09804189413777487, Accuracy: 0.9361702127659575\n",
            "Batch 2 - Loss: 0.10711065065738128, Accuracy: 0.9574468085106383\n",
            "Batch 3 - Loss: 0.37915957231915626, Accuracy: 0.8947368421052632\n",
            "Batch 1 - Loss: 0.19639240688565876, Accuracy: 0.9452054794520548\n",
            "Batch 2 - Loss: 0.16439669324143116, Accuracy: 0.9178082191780822\n",
            "Batch 3 - Loss: 0.1259482292973288, Accuracy: 0.9285714285714286\n",
            "Batch 1 - Loss: 0.09628345233937535, Accuracy: 0.9315068493150684\n",
            "Batch 2 - Loss: 0.1572303358991295, Accuracy: 0.9178082191780822\n",
            "Batch 3 - Loss: 0.12452895929313043, Accuracy: 0.9285714285714286\n",
            "Batch 1 - Loss: 0.09665359561709401, Accuracy: 0.9315068493150684\n",
            "Batch 2 - Loss: 0.151480420337303, Accuracy: 0.9041095890410958\n",
            "Batch 3 - Loss: 0.12547374599235858, Accuracy: 0.9285714285714286\n",
            "Batch 1 - Loss: 0.09672199014181715, Accuracy: 0.9315068493150684\n",
            "Batch 2 - Loss: 0.14702125663707313, Accuracy: 0.9041095890410958\n",
            "Batch 3 - Loss: 0.1262499672911736, Accuracy: 0.9285714285714286\n",
            "Batch 1 - Loss: 0.09744862281233252, Accuracy: 0.9315068493150684\n",
            "Batch 2 - Loss: 0.14495135177987234, Accuracy: 0.9041095890410958\n",
            "Batch 3 - Loss: 0.12680141409738663, Accuracy: 0.9285714285714286\n",
            "Batch 1 - Loss: 0.0974913604181902, Accuracy: 0.9315068493150684\n",
            "Batch 2 - Loss: 0.14266697756503505, Accuracy: 0.9041095890410958\n",
            "Batch 3 - Loss: 0.12787067464041108, Accuracy: 0.9285714285714286\n",
            "Batch 1 - Loss: 0.09822986021386648, Accuracy: 0.9315068493150684\n",
            "Batch 2 - Loss: 0.14173793879632188, Accuracy: 0.9041095890410958\n",
            "Batch 3 - Loss: 0.12810716824770982, Accuracy: 0.9285714285714286\n",
            "Batch 1 - Loss: 0.09886109355078175, Accuracy: 0.9315068493150684\n",
            "Batch 2 - Loss: 0.14121901244959717, Accuracy: 0.9041095890410958\n",
            "Batch 3 - Loss: 0.12847833259462493, Accuracy: 0.9285714285714286\n",
            "Batch 1 - Loss: 0.10268796430558103, Accuracy: 0.9393939393939394\n",
            "Batch 1 - Loss: 0.12966885129737166, Accuracy: 0.9393939393939394\n",
            "Batch 2 - Loss: 0.16226777613441706, Accuracy: 0.9090909090909091\n",
            "Batch 2 - Loss: 0.05243746886865791, Accuracy: 0.9696969696969697\n",
            "Batch 3 - Loss: 0.24856323233458638, Accuracy: 0.9285714285714286\n",
            "Batch 1 - Loss: 0.11647000205634468, Accuracy: 0.9393939393939394\n",
            "Batch 1 - Loss: 0.1227490260584559, Accuracy: 0.9393939393939394\n",
            "Batch 2 - Loss: 0.15986150932490462, Accuracy: 0.9090909090909091\n",
            "Batch 2 - Loss: 0.05243750492226676, Accuracy: 0.9696969696969697\n",
            "Batch 3 - Loss: 0.24770745978683076, Accuracy: 0.9285714285714286\n",
            "Batch 1 - Loss: 0.11578449461104802, Accuracy: 0.9393939393939394\n",
            "Batch 1 - Loss: 0.12276390704593795, Accuracy: 0.9393939393939394\n",
            "Batch 2 - Loss: 0.15950972905968283, Accuracy: 0.9090909090909091\n",
            "Batch 2 - Loss: 0.05289296359068166, Accuracy: 0.9696969696969697\n",
            "Batch 3 - Loss: 0.2467046705508434, Accuracy: 0.9285714285714286\n",
            "Batch 1 - Loss: 0.1151220637657114, Accuracy: 0.9393939393939394\n",
            "Batch 1 - Loss: 0.12272660559450893, Accuracy: 0.9393939393939394\n",
            "Batch 2 - Loss: 0.15920967847647338, Accuracy: 0.9090909090909091\n",
            "Batch 2 - Loss: 0.053286369095630115, Accuracy: 0.9696969696969697\n",
            "Batch 3 - Loss: 0.2459110188081476, Accuracy: 0.9285714285714286\n",
            "Batch 1 - Loss: 0.1145540341798247, Accuracy: 0.9393939393939394\n",
            "Batch 1 - Loss: 0.12266047153567675, Accuracy: 0.9393939393939394\n",
            "Batch 2 - Loss: 0.15895395217147365, Accuracy: 0.9090909090909091\n",
            "Batch 2 - Loss: 0.05361189355308777, Accuracy: 0.9696969696969697\n",
            "Batch 3 - Loss: 0.24526657364083673, Accuracy: 0.9285714285714286\n",
            "Batch 1 - Loss: 0.11407217545325649, Accuracy: 0.9393939393939394\n",
            "Batch 1 - Loss: 0.12258252156810061, Accuracy: 0.9393939393939394\n",
            "Batch 2 - Loss: 0.15873593017873006, Accuracy: 0.9090909090909091\n",
            "Batch 2 - Loss: 0.05387748427645487, Accuracy: 0.9696969696969697\n",
            "Batch 3 - Loss: 0.24526521146356753, Accuracy: 0.9285714285714286\n",
            "Batch 1 - Loss: 0.11371091960834441, Accuracy: 0.9393939393939394\n",
            "Batch 1 - Loss: 0.12253990347642049, Accuracy: 0.9696969696969697\n",
            "Batch 2 - Loss: 0.1584525364444304, Accuracy: 0.9090909090909091\n",
            "Batch 2 - Loss: 0.054142086832709324, Accuracy: 0.9696969696969697\n",
            "Batch 3 - Loss: 0.2442483695849027, Accuracy: 0.9285714285714286\n",
            "Batch 1 - Loss: 0.11326924727038312, Accuracy: 0.9393939393939394\n",
            "Batch 1 - Loss: 0.12246831252889807, Accuracy: 0.9696969696969697\n",
            "Batch 2 - Loss: 0.15831007892178087, Accuracy: 0.9090909090909091\n",
            "Batch 2 - Loss: 0.054298527491491534, Accuracy: 0.9696969696969697\n",
            "Batch 3 - Loss: 0.2438867120913355, Accuracy: 0.9285714285714286\n",
            "Batch 1 - Loss: 0.1272997527376975, Accuracy: 0.9347826086956522\n",
            "Batch 1 - Loss: 0.09553220176488618, Accuracy: 0.9565217391304348\n",
            "Batch 2 - Loss: 0.05998028350720601, Accuracy: 0.9782608695652174\n",
            "Batch 3 - Loss: 0.2783304434945981, Accuracy: 0.9090909090909091\n",
            "Batch 1 - Loss: 0.14610760645725981, Accuracy: 0.9347826086956522\n",
            "Batch 1 - Loss: 0.093741138893488, Accuracy: 0.9565217391304348\n",
            "Batch 2 - Loss: 0.059554859140650236, Accuracy: 0.9782608695652174\n",
            "Batch 3 - Loss: 0.280122295722331, Accuracy: 0.9090909090909091\n",
            "Batch 1 - Loss: 0.14559156722857272, Accuracy: 0.9347826086956522\n",
            "Batch 1 - Loss: 0.09465265702810069, Accuracy: 0.9565217391304348\n",
            "Batch 2 - Loss: 0.060301585356231174, Accuracy: 0.9782608695652174\n",
            "Batch 3 - Loss: 0.2803013342340121, Accuracy: 0.9090909090909091\n",
            "Batch 1 - Loss: 0.14503509791907862, Accuracy: 0.9347826086956522\n",
            "Batch 1 - Loss: 0.09509616200975055, Accuracy: 0.9565217391304348\n",
            "Batch 2 - Loss: 0.06072716789936959, Accuracy: 0.9782608695652174\n",
            "Batch 3 - Loss: 0.2805851308769655, Accuracy: 0.9090909090909091\n",
            "Batch 1 - Loss: 0.14473251103358978, Accuracy: 0.9347826086956522\n",
            "Batch 1 - Loss: 0.09518332894999641, Accuracy: 0.9565217391304348\n",
            "Batch 2 - Loss: 0.06092655403741287, Accuracy: 0.9782608695652174\n",
            "Batch 3 - Loss: 0.2809739820799851, Accuracy: 0.9090909090909091\n",
            "Batch 1 - Loss: 0.1445724767754322, Accuracy: 0.9347826086956522\n",
            "Batch 1 - Loss: 0.09509857823679477, Accuracy: 0.9565217391304348\n",
            "Batch 2 - Loss: 0.061015466393446155, Accuracy: 0.9782608695652174\n",
            "Batch 3 - Loss: 0.2814000892988479, Accuracy: 0.9090909090909091\n",
            "Batch 1 - Loss: 0.14448289048061527, Accuracy: 0.9347826086956522\n",
            "Batch 1 - Loss: 0.09493661153905494, Accuracy: 0.9565217391304348\n",
            "Batch 2 - Loss: 0.061052929922013985, Accuracy: 0.9782608695652174\n",
            "Batch 3 - Loss: 0.2818293595205255, Accuracy: 0.9090909090909091\n",
            "Batch 1 - Loss: 0.14442807538282668, Accuracy: 0.9347826086956522\n",
            "Batch 1 - Loss: 0.09474282566687753, Accuracy: 0.9565217391304348\n",
            "Batch 2 - Loss: 0.0610669731647654, Accuracy: 0.9782608695652174\n",
            "Batch 3 - Loss: 0.28224681529665063, Accuracy: 0.9090909090909091\n",
            "Batch 1 - Loss: 0.13226146441108114, Accuracy: 0.9393939393939394\n",
            "Batch 2 - Loss: 0.12152087513989066, Accuracy: 0.9242424242424242\n",
            "Batch 3 - Loss: 0.20144276276624437, Accuracy: 0.9285714285714286\n",
            "Batch 1 - Loss: 0.10409326721785202, Accuracy: 0.9242424242424242\n",
            "Batch 2 - Loss: 0.11717343674280906, Accuracy: 0.9242424242424242\n",
            "Batch 3 - Loss: 0.20430112057417324, Accuracy: 0.9285714285714286\n",
            "Batch 1 - Loss: 0.10499147924388705, Accuracy: 0.9393939393939394\n",
            "Batch 2 - Loss: 0.11694473332058411, Accuracy: 0.9242424242424242\n",
            "Batch 3 - Loss: 0.20479172901913842, Accuracy: 0.9285714285714286\n",
            "Batch 1 - Loss: 0.10529864798033166, Accuracy: 0.9393939393939394\n",
            "Batch 2 - Loss: 0.11654510775064081, Accuracy: 0.9242424242424242\n",
            "Batch 3 - Loss: 0.2052640106408534, Accuracy: 0.9285714285714286\n",
            "Batch 1 - Loss: 0.10552404482060802, Accuracy: 0.9393939393939394\n",
            "Batch 2 - Loss: 0.11626468984002909, Accuracy: 0.9242424242424242\n",
            "Batch 3 - Loss: 0.20557881550779372, Accuracy: 0.9285714285714286\n",
            "Batch 1 - Loss: 0.10567206457584548, Accuracy: 0.9393939393939394\n",
            "Batch 2 - Loss: 0.11605515887185748, Accuracy: 0.9242424242424242\n",
            "Batch 3 - Loss: 0.20578836176349244, Accuracy: 0.9285714285714286\n",
            "Batch 1 - Loss: 0.10576727912691986, Accuracy: 0.9393939393939394\n",
            "Batch 2 - Loss: 0.11589368398401424, Accuracy: 0.9242424242424242\n",
            "Batch 3 - Loss: 0.20592547678721163, Accuracy: 0.9285714285714286\n",
            "Batch 1 - Loss: 0.1058264663916032, Accuracy: 0.9393939393939394\n",
            "Batch 2 - Loss: 0.11576387042381277, Accuracy: 0.9242424242424242\n",
            "Batch 3 - Loss: 0.20601380350481793, Accuracy: 0.9285714285714286\n",
            "Batch 1 - Loss: 0.12495177707648704, Accuracy: 0.9342105263157895\n",
            "Batch 2 - Loss: 0.10629903552729925, Accuracy: 0.9342105263157895\n",
            "Batch 3 - Loss: 0.19057857912653398, Accuracy: 0.875\n",
            "Batch 1 - Loss: 0.21059585744278064, Accuracy: 0.9342105263157895\n",
            "Batch 2 - Loss: 0.09988500347910745, Accuracy: 0.9605263157894737\n",
            "Batch 3 - Loss: 0.1939567657778854, Accuracy: 0.875\n",
            "Batch 1 - Loss: 0.19420494716583553, Accuracy: 0.9342105263157895\n",
            "Batch 2 - Loss: 0.1336360213836621, Accuracy: 0.9342105263157895\n",
            "Batch 3 - Loss: 0.16149745336048693, Accuracy: 0.875\n",
            "Batch 1 - Loss: 0.1735158312918912, Accuracy: 0.9342105263157895\n",
            "Batch 2 - Loss: 0.14548508076439023, Accuracy: 0.9342105263157895\n",
            "Batch 3 - Loss: 0.14895622288240754, Accuracy: 0.875\n",
            "Batch 1 - Loss: 0.1667115070376108, Accuracy: 0.9342105263157895\n",
            "Batch 2 - Loss: 0.1450442926253551, Accuracy: 0.9342105263157895\n",
            "Batch 3 - Loss: 0.1474495630938003, Accuracy: 0.875\n",
            "Batch 1 - Loss: 0.16725006430137532, Accuracy: 0.9342105263157895\n",
            "Batch 2 - Loss: 0.1452828678507611, Accuracy: 0.9342105263157895\n",
            "Batch 3 - Loss: 0.14578449465529802, Accuracy: 0.875\n",
            "Batch 1 - Loss: 0.16639997082912117, Accuracy: 0.9342105263157895\n",
            "Batch 2 - Loss: 0.14500529316140884, Accuracy: 0.9342105263157895\n",
            "Batch 3 - Loss: 0.14459369598081262, Accuracy: 0.875\n",
            "Batch 1 - Loss: 0.16641373897993814, Accuracy: 0.9342105263157895\n",
            "Batch 2 - Loss: 0.14503672316255367, Accuracy: 0.9342105263157895\n",
            "Batch 3 - Loss: 0.1432361614538036, Accuracy: 0.875\n",
            "Batch 1 - Loss: 0.14823099853687335, Accuracy: 0.9428571428571428\n",
            "Batch 1 - Loss: 0.09970242745814306, Accuracy: 0.9428571428571428\n",
            "Batch 2 - Loss: 0.2225601684857534, Accuracy: 0.8857142857142857\n",
            "Batch 2 - Loss: 0.03856741345143225, Accuracy: 1.0\n",
            "Batch 3 - Loss: 0.34165562954563844, Accuracy: 0.9\n",
            "Batch 1 - Loss: 0.21580171830079162, Accuracy: 0.9428571428571428\n",
            "Batch 1 - Loss: 0.12033382887667839, Accuracy: 0.9714285714285714\n",
            "Batch 2 - Loss: 0.2229169825581195, Accuracy: 0.8857142857142857\n",
            "Batch 2 - Loss: 0.03781330247513483, Accuracy: 1.0\n",
            "Batch 3 - Loss: 0.3449731294988211, Accuracy: 0.9\n",
            "Batch 1 - Loss: 0.20316620849064287, Accuracy: 0.9428571428571428\n",
            "Batch 1 - Loss: 0.10897256494553885, Accuracy: 0.9714285714285714\n",
            "Batch 2 - Loss: 0.2197377878102517, Accuracy: 0.8857142857142857\n",
            "Batch 2 - Loss: 0.038452546774900856, Accuracy: 1.0\n",
            "Batch 3 - Loss: 0.341134497097529, Accuracy: 0.9\n",
            "Batch 1 - Loss: 0.19996282827900838, Accuracy: 0.9428571428571428\n",
            "Batch 1 - Loss: 0.10791665614099114, Accuracy: 0.9714285714285714\n",
            "Batch 2 - Loss: 0.21741635923123223, Accuracy: 0.8857142857142857\n",
            "Batch 2 - Loss: 0.03887268764714578, Accuracy: 1.0\n",
            "Batch 3 - Loss: 0.33848387220333354, Accuracy: 0.9\n",
            "Batch 1 - Loss: 0.1974887081576587, Accuracy: 0.9428571428571428\n",
            "Batch 1 - Loss: 0.10693471717333031, Accuracy: 0.9714285714285714\n",
            "Batch 2 - Loss: 0.2159195353038259, Accuracy: 0.8857142857142857\n",
            "Batch 2 - Loss: 0.03918391068092155, Accuracy: 1.0\n",
            "Batch 3 - Loss: 0.33639834986890593, Accuracy: 0.9\n",
            "Batch 1 - Loss: 0.1954576452672512, Accuracy: 0.9428571428571428\n",
            "Batch 1 - Loss: 0.10600860135206247, Accuracy: 0.9714285714285714\n",
            "Batch 2 - Loss: 0.21490564523855213, Accuracy: 0.8857142857142857\n",
            "Batch 2 - Loss: 0.0394143911074877, Accuracy: 1.0\n",
            "Batch 3 - Loss: 0.33447073124909515, Accuracy: 0.9\n",
            "Batch 1 - Loss: 0.19275760905625683, Accuracy: 0.9428571428571428\n",
            "Batch 1 - Loss: 0.10509815317040824, Accuracy: 0.9714285714285714\n",
            "Batch 2 - Loss: 0.21372103792710695, Accuracy: 0.8857142857142857\n",
            "Batch 2 - Loss: 0.03979320314048239, Accuracy: 1.0\n",
            "Batch 3 - Loss: 0.33285446601767565, Accuracy: 0.9\n",
            "Batch 1 - Loss: 0.19219452165785808, Accuracy: 0.9428571428571428\n",
            "Batch 1 - Loss: 0.10454688249699365, Accuracy: 0.9714285714285714\n",
            "Batch 2 - Loss: 0.21336683433279768, Accuracy: 0.8857142857142857\n",
            "Batch 2 - Loss: 0.03996552719122214, Accuracy: 1.0\n",
            "Batch 3 - Loss: 0.33466582619656626, Accuracy: 0.9\n",
            "Batch 1 - Loss: 0.2140053087836535, Accuracy: 0.9302325581395349\n",
            "Batch 2 - Loss: 0.12154710701083773, Accuracy: 0.9459459459459459\n",
            "Batch 1 - Loss: 0.0987792310550283, Accuracy: 0.9651162790697675\n",
            "Batch 2 - Loss: 0.10254809295896794, Accuracy: 0.9459459459459459\n",
            "Batch 1 - Loss: 0.09875201584285923, Accuracy: 0.9651162790697675\n",
            "Batch 2 - Loss: 0.10008434151774435, Accuracy: 0.9459459459459459\n",
            "Batch 1 - Loss: 0.0985063251670944, Accuracy: 0.9651162790697675\n",
            "Batch 2 - Loss: 0.09842297712737029, Accuracy: 0.9459459459459459\n",
            "Batch 1 - Loss: 0.09825164149270575, Accuracy: 0.9651162790697675\n",
            "Batch 2 - Loss: 0.09708750918752548, Accuracy: 0.9459459459459459\n",
            "Batch 1 - Loss: 0.09803455358726154, Accuracy: 0.9651162790697675\n",
            "Batch 2 - Loss: 0.09601630921572767, Accuracy: 0.9459459459459459\n",
            "Batch 1 - Loss: 0.09782475585541574, Accuracy: 0.9651162790697675\n",
            "Batch 2 - Loss: 0.09508757930984203, Accuracy: 0.9459459459459459\n",
            "Batch 1 - Loss: 0.09764262009379111, Accuracy: 0.9651162790697675\n",
            "Batch 2 - Loss: 0.094330795411731, Accuracy: 0.9459459459459459\n",
            "Batch 1 - Loss: 0.09639248321295676, Accuracy: 0.9655172413793104\n",
            "Batch 2 - Loss: 0.09492500948414206, Accuracy: 0.9452054794520548\n",
            "Batch 1 - Loss: 0.09614862683330862, Accuracy: 0.9655172413793104\n",
            "Batch 2 - Loss: 0.09445281078235773, Accuracy: 0.9452054794520548\n",
            "Batch 1 - Loss: 0.09602217191433678, Accuracy: 0.9655172413793104\n",
            "Batch 2 - Loss: 0.09404378073405611, Accuracy: 0.9452054794520548\n",
            "Batch 1 - Loss: 0.09586944415157098, Accuracy: 0.9655172413793104\n",
            "Batch 2 - Loss: 0.09360637834298562, Accuracy: 0.9452054794520548\n",
            "Batch 1 - Loss: 0.09572015878281527, Accuracy: 0.9655172413793104\n",
            "Batch 2 - Loss: 0.09329605484201427, Accuracy: 0.9452054794520548\n",
            "Batch 1 - Loss: 0.09558020832029934, Accuracy: 0.9655172413793104\n",
            "Batch 2 - Loss: 0.09289039224677999, Accuracy: 0.9452054794520548\n",
            "Batch 1 - Loss: 0.09541875155622478, Accuracy: 0.9655172413793104\n",
            "Batch 2 - Loss: 0.09263781472440408, Accuracy: 0.9452054794520548\n",
            "Batch 1 - Loss: 0.09530848864001809, Accuracy: 0.9655172413793104\n",
            "Batch 2 - Loss: 0.0923085530510799, Accuracy: 0.9452054794520548\n",
            "Batch 1 - Loss: 0.11262420863228989, Accuracy: 0.9534883720930233\n",
            "Batch 1 - Loss: 0.10854589373714756, Accuracy: 0.9534883720930233\n",
            "Batch 2 - Loss: 0.04162461016107671, Accuracy: 1.0\n",
            "Batch 3 - Loss: 0.23233424455027632, Accuracy: 0.9354838709677419\n",
            "Batch 1 - Loss: 0.11303248927323485, Accuracy: 0.9534883720930233\n",
            "Batch 1 - Loss: 0.12100020897870017, Accuracy: 0.9534883720930233\n",
            "Batch 2 - Loss: 0.04702589148500225, Accuracy: 0.9767441860465116\n",
            "Batch 3 - Loss: 0.23069800609440416, Accuracy: 0.9354838709677419\n",
            "Batch 1 - Loss: 0.10910982779324065, Accuracy: 0.9534883720930233\n",
            "Batch 1 - Loss: 0.12440887917654017, Accuracy: 0.9534883720930233\n",
            "Batch 2 - Loss: 0.050163089106974715, Accuracy: 0.9767441860465116\n",
            "Batch 3 - Loss: 0.23184918323602705, Accuracy: 0.9354838709677419\n",
            "Batch 1 - Loss: 0.10754312931005665, Accuracy: 0.9534883720930233\n",
            "Batch 1 - Loss: 0.12513410615395934, Accuracy: 0.9534883720930233\n",
            "Batch 2 - Loss: 0.0520549994137293, Accuracy: 0.9767441860465116\n",
            "Batch 3 - Loss: 0.23154282407330146, Accuracy: 0.9354838709677419\n",
            "Batch 1 - Loss: 0.1065340158017465, Accuracy: 0.9534883720930233\n",
            "Batch 1 - Loss: 0.1251446320861585, Accuracy: 0.9534883720930233\n",
            "Batch 2 - Loss: 0.05329875514343307, Accuracy: 0.9767441860465116\n",
            "Batch 3 - Loss: 0.22994937508177965, Accuracy: 0.9354838709677419\n",
            "Batch 1 - Loss: 0.1059059743959895, Accuracy: 0.9534883720930233\n",
            "Batch 1 - Loss: 0.12497336530496737, Accuracy: 0.9534883720930233\n",
            "Batch 2 - Loss: 0.05388537180344683, Accuracy: 0.9767441860465116\n",
            "Batch 3 - Loss: 0.22958103240117828, Accuracy: 0.9354838709677419\n",
            "Batch 1 - Loss: 0.10559360059630614, Accuracy: 0.9534883720930233\n",
            "Batch 1 - Loss: 0.1248409643479409, Accuracy: 0.9534883720930233\n",
            "Batch 2 - Loss: 0.054329676577590656, Accuracy: 0.9767441860465116\n",
            "Batch 3 - Loss: 0.2324058314239542, Accuracy: 0.9354838709677419\n",
            "Batch 1 - Loss: 0.10554275853297755, Accuracy: 0.9534883720930233\n",
            "Batch 1 - Loss: 0.12463419105742826, Accuracy: 0.9534883720930233\n",
            "Batch 2 - Loss: 0.054774320237077505, Accuracy: 0.9767441860465116\n",
            "Batch 3 - Loss: 0.23016653413574323, Accuracy: 0.9354838709677419\n",
            "Batch 1 - Loss: 0.1198958235576836, Accuracy: 0.9574468085106383\n",
            "Batch 1 - Loss: 0.09285473026524654, Accuracy: 0.9574468085106383\n",
            "Batch 2 - Loss: 0.08078429200908427, Accuracy: 0.9574468085106383\n",
            "Batch 3 - Loss: 0.38676358634844693, Accuracy: 0.8947368421052632\n",
            "Batch 1 - Loss: 0.27988662891245675, Accuracy: 0.8723404255319149\n",
            "Batch 1 - Loss: 0.036572813921944405, Accuracy: 0.9787234042553191\n",
            "Batch 2 - Loss: 0.048072812554875326, Accuracy: 0.9787234042553191\n",
            "Batch 3 - Loss: 0.5408791071413265, Accuracy: 0.8947368421052632\n",
            "Batch 1 - Loss: 0.20687023745939156, Accuracy: 0.9148936170212766\n",
            "Batch 1 - Loss: 0.1308892938923421, Accuracy: 0.9361702127659575\n",
            "Batch 2 - Loss: 0.1458734283477963, Accuracy: 0.9361702127659575\n",
            "Batch 3 - Loss: 0.36106083129231403, Accuracy: 0.8947368421052632\n",
            "Batch 1 - Loss: 0.198768696081013, Accuracy: 0.9361702127659575\n",
            "Batch 1 - Loss: 0.10554682824551895, Accuracy: 0.9361702127659575\n",
            "Batch 2 - Loss: 0.11482003348568984, Accuracy: 0.9574468085106383\n",
            "Batch 3 - Loss: 0.38043243638302743, Accuracy: 0.8947368421052632\n",
            "Batch 1 - Loss: 0.20271497890387435, Accuracy: 0.9361702127659575\n",
            "Batch 1 - Loss: 0.09583518187952804, Accuracy: 0.9361702127659575\n",
            "Batch 2 - Loss: 0.10526515574761013, Accuracy: 0.9574468085106383\n",
            "Batch 3 - Loss: 0.3897195367257521, Accuracy: 0.8947368421052632\n",
            "Batch 1 - Loss: 0.20097242345383995, Accuracy: 0.9361702127659575\n",
            "Batch 1 - Loss: 0.09578956745949008, Accuracy: 0.9361702127659575\n",
            "Batch 2 - Loss: 0.10610250057510902, Accuracy: 0.9574468085106383\n",
            "Batch 3 - Loss: 0.38496373101907166, Accuracy: 0.8947368421052632\n",
            "Batch 1 - Loss: 0.19774824047066675, Accuracy: 0.9361702127659575\n",
            "Batch 1 - Loss: 0.09679586479548272, Accuracy: 0.9361702127659575\n",
            "Batch 2 - Loss: 0.1077592969068687, Accuracy: 0.9574468085106383\n",
            "Batch 3 - Loss: 0.37838140653421926, Accuracy: 0.8947368421052632\n",
            "Batch 1 - Loss: 0.1950933945069493, Accuracy: 0.9361702127659575\n",
            "Batch 1 - Loss: 0.09721256984397861, Accuracy: 0.9361702127659575\n",
            "Batch 2 - Loss: 0.10863105446120729, Accuracy: 0.9574468085106383\n",
            "Batch 3 - Loss: 0.37337545765450453, Accuracy: 0.8947368421052632\n",
            "Batch 1 - Loss: 0.18904254002565665, Accuracy: 0.9452054794520548\n",
            "Batch 2 - Loss: 0.16378392639031908, Accuracy: 0.9178082191780822\n",
            "Batch 3 - Loss: 0.12330986396066271, Accuracy: 0.9285714285714286\n",
            "Batch 1 - Loss: 0.09106972231348864, Accuracy: 0.9452054794520548\n",
            "Batch 2 - Loss: 0.158741433421289, Accuracy: 0.9178082191780822\n",
            "Batch 3 - Loss: 0.1211802068739251, Accuracy: 0.9285714285714286\n",
            "Batch 1 - Loss: 0.09066061600990132, Accuracy: 0.9452054794520548\n",
            "Batch 2 - Loss: 0.15158705948976442, Accuracy: 0.9041095890410958\n",
            "Batch 3 - Loss: 0.12326945634161693, Accuracy: 0.9285714285714286\n",
            "Batch 1 - Loss: 0.09172478047559099, Accuracy: 0.9452054794520548\n",
            "Batch 2 - Loss: 0.14858727846435096, Accuracy: 0.9041095890410958\n",
            "Batch 3 - Loss: 0.12317867416973367, Accuracy: 0.9285714285714286\n",
            "Batch 1 - Loss: 0.09131787162095692, Accuracy: 0.9452054794520548\n",
            "Batch 2 - Loss: 0.14476643519599497, Accuracy: 0.9041095890410958\n",
            "Batch 3 - Loss: 0.12489332399001718, Accuracy: 0.9285714285714286\n",
            "Batch 1 - Loss: 0.09256322884145611, Accuracy: 0.9452054794520548\n",
            "Batch 2 - Loss: 0.14397215121558124, Accuracy: 0.9041095890410958\n",
            "Batch 3 - Loss: 0.12508154580557618, Accuracy: 0.9285714285714286\n",
            "Batch 1 - Loss: 0.09282637707468343, Accuracy: 0.9452054794520548\n",
            "Batch 2 - Loss: 0.14256427418267342, Accuracy: 0.9041095890410958\n",
            "Batch 3 - Loss: 0.1258348698316135, Accuracy: 0.9285714285714286\n",
            "Batch 1 - Loss: 0.09259404918743092, Accuracy: 0.9452054794520548\n",
            "Batch 2 - Loss: 0.14068597567936422, Accuracy: 0.9041095890410958\n",
            "Batch 3 - Loss: 0.1265567348125443, Accuracy: 0.9285714285714286\n",
            "Batch 1 - Loss: 0.09367448558668731, Accuracy: 0.9452054794520548\n",
            "Batch 2 - Loss: 0.14073888265626353, Accuracy: 0.9041095890410958\n",
            "Batch 3 - Loss: 0.12648033756061952, Accuracy: 0.9285714285714286\n",
            "Batch 1 - Loss: 0.1027602098604447, Accuracy: 0.9393939393939394\n",
            "Batch 1 - Loss: 0.1202131610599806, Accuracy: 0.9696969696969697\n",
            "Batch 2 - Loss: 0.1624791496530743, Accuracy: 0.9090909090909091\n",
            "Batch 2 - Loss: 0.052997281326865944, Accuracy: 0.9696969696969697\n",
            "Batch 3 - Loss: 0.24713393457453417, Accuracy: 0.9285714285714286\n",
            "Batch 1 - Loss: 0.11626196237002313, Accuracy: 0.9393939393939394\n",
            "Batch 1 - Loss: 0.11369923621373923, Accuracy: 0.9696969696969697\n",
            "Batch 2 - Loss: 0.15963947327062555, Accuracy: 0.9090909090909091\n",
            "Batch 2 - Loss: 0.05253733453647804, Accuracy: 0.9696969696969697\n",
            "Batch 3 - Loss: 0.24644477703524875, Accuracy: 0.9285714285714286\n",
            "Batch 1 - Loss: 0.11551255273631493, Accuracy: 0.9393939393939394\n",
            "Batch 1 - Loss: 0.11405704090292673, Accuracy: 0.9696969696969697\n",
            "Batch 2 - Loss: 0.1591129399703583, Accuracy: 0.9090909090909091\n",
            "Batch 2 - Loss: 0.05288540308000288, Accuracy: 0.9696969696969697\n",
            "Batch 3 - Loss: 0.24537680783830354, Accuracy: 0.9285714285714286\n",
            "Batch 1 - Loss: 0.1147642360750107, Accuracy: 0.9393939393939394\n",
            "Batch 1 - Loss: 0.11426499516818805, Accuracy: 0.9696969696969697\n",
            "Batch 2 - Loss: 0.15870483612294287, Accuracy: 0.9090909090909091\n",
            "Batch 2 - Loss: 0.05322094028875454, Accuracy: 0.9696969696969697\n",
            "Batch 3 - Loss: 0.24453524814258826, Accuracy: 0.9285714285714286\n",
            "Batch 1 - Loss: 0.11412230507765822, Accuracy: 0.9393939393939394\n",
            "Batch 1 - Loss: 0.11438634808104921, Accuracy: 0.9696969696969697\n",
            "Batch 2 - Loss: 0.15837581564495404, Accuracy: 0.9090909090909091\n",
            "Batch 2 - Loss: 0.053513976404468845, Accuracy: 0.9696969696969697\n",
            "Batch 3 - Loss: 0.24385665649944313, Accuracy: 0.9285714285714286\n",
            "Batch 1 - Loss: 0.11357789265857009, Accuracy: 0.9393939393939394\n",
            "Batch 1 - Loss: 0.1144595548338769, Accuracy: 0.9696969696969697\n",
            "Batch 2 - Loss: 0.1581075289316618, Accuracy: 0.9090909090909091\n",
            "Batch 2 - Loss: 0.05375893741976901, Accuracy: 0.9696969696969697\n",
            "Batch 3 - Loss: 0.24330054334687956, Accuracy: 0.9285714285714286\n",
            "Batch 1 - Loss: 0.11311877516227235, Accuracy: 0.9393939393939394\n",
            "Batch 1 - Loss: 0.11450330303750864, Accuracy: 0.9696969696969697\n",
            "Batch 2 - Loss: 0.15788663699748332, Accuracy: 0.9090909090909091\n",
            "Batch 2 - Loss: 0.053958259432777, Accuracy: 0.9696969696969697\n",
            "Batch 3 - Loss: 0.24283940295673914, Accuracy: 0.9285714285714286\n",
            "Batch 1 - Loss: 0.11273212440772679, Accuracy: 0.9393939393939394\n",
            "Batch 1 - Loss: 0.11452783479987098, Accuracy: 0.9696969696969697\n",
            "Batch 2 - Loss: 0.15770305842821905, Accuracy: 0.9090909090909091\n",
            "Batch 2 - Loss: 0.05412392119967779, Accuracy: 0.9696969696969697\n",
            "Batch 3 - Loss: 0.2428662918200235, Accuracy: 0.9285714285714286\n",
            "Batch 1 - Loss: 0.11244008634703051, Accuracy: 0.9393939393939394\n",
            "Batch 1 - Loss: 0.11457028112078642, Accuracy: 0.9696969696969697\n",
            "Batch 2 - Loss: 0.15747656777474323, Accuracy: 0.9090909090909091\n",
            "Batch 2 - Loss: 0.05428809364837334, Accuracy: 0.9696969696969697\n",
            "Batch 3 - Loss: 0.24209776278041187, Accuracy: 0.9285714285714286\n",
            "Batch 1 - Loss: 0.12160178137412374, Accuracy: 0.9565217391304348\n",
            "Batch 1 - Loss: 0.09415525651714651, Accuracy: 0.9565217391304348\n",
            "Batch 2 - Loss: 0.058836257655841016, Accuracy: 0.9782608695652174\n",
            "Batch 3 - Loss: 0.2755023088341063, Accuracy: 0.9090909090909091\n",
            "Batch 1 - Loss: 0.14005234554413737, Accuracy: 0.9565217391304348\n",
            "Batch 1 - Loss: 0.09285581122300629, Accuracy: 0.9565217391304348\n",
            "Batch 2 - Loss: 0.05845434917465645, Accuracy: 0.9782608695652174\n",
            "Batch 3 - Loss: 0.2791330685570759, Accuracy: 0.9090909090909091\n",
            "Batch 1 - Loss: 0.13995492018423517, Accuracy: 0.9565217391304348\n",
            "Batch 1 - Loss: 0.0935790764436432, Accuracy: 0.9565217391304348\n",
            "Batch 2 - Loss: 0.05920529972248002, Accuracy: 0.9782608695652174\n",
            "Batch 3 - Loss: 0.2793344481805794, Accuracy: 0.9090909090909091\n",
            "Batch 1 - Loss: 0.13930591264147127, Accuracy: 0.9565217391304348\n",
            "Batch 1 - Loss: 0.09411336311414012, Accuracy: 0.9565217391304348\n",
            "Batch 2 - Loss: 0.05973217754587389, Accuracy: 0.9782608695652174\n",
            "Batch 3 - Loss: 0.2795150809489442, Accuracy: 0.9090909090909091\n",
            "Batch 1 - Loss: 0.13891869799251527, Accuracy: 0.9565217391304348\n",
            "Batch 1 - Loss: 0.09428178458409771, Accuracy: 0.9565217391304348\n",
            "Batch 2 - Loss: 0.060000001743709945, Accuracy: 0.9782608695652174\n",
            "Batch 3 - Loss: 0.27982306268745794, Accuracy: 0.9090909090909091\n",
            "Batch 1 - Loss: 0.13870852607889905, Accuracy: 0.9565217391304348\n",
            "Batch 1 - Loss: 0.09425491858646914, Accuracy: 0.9565217391304348\n",
            "Batch 2 - Loss: 0.06012976230039027, Accuracy: 0.9782608695652174\n",
            "Batch 3 - Loss: 0.28019219957392677, Accuracy: 0.9090909090909091\n",
            "Batch 1 - Loss: 0.13859241960309815, Accuracy: 0.9565217391304348\n",
            "Batch 1 - Loss: 0.09413441740515878, Accuracy: 0.9565217391304348\n",
            "Batch 2 - Loss: 0.060191287857647574, Accuracy: 0.9782608695652174\n",
            "Batch 3 - Loss: 0.280583621030513, Accuracy: 0.9090909090909091\n",
            "Batch 1 - Loss: 0.13828389238299507, Accuracy: 0.9565217391304348\n",
            "Batch 1 - Loss: 0.09379320593656863, Accuracy: 0.9565217391304348\n",
            "Batch 2 - Loss: 0.060148168803287216, Accuracy: 0.9782608695652174\n",
            "Batch 3 - Loss: 0.2810191895021894, Accuracy: 0.9090909090909091\n",
            "Batch 1 - Loss: 0.13851245510205465, Accuracy: 0.9565217391304348\n",
            "Batch 1 - Loss: 0.09358379737461789, Accuracy: 0.9565217391304348\n",
            "Batch 2 - Loss: 0.06014570479371491, Accuracy: 0.9782608695652174\n",
            "Batch 3 - Loss: 0.2814531097377459, Accuracy: 0.9090909090909091\n",
            "Batch 1 - Loss: 0.12745906932617262, Accuracy: 0.9545454545454546\n",
            "Batch 2 - Loss: 0.12031984332840734, Accuracy: 0.9242424242424242\n",
            "Batch 3 - Loss: 0.20056688096175307, Accuracy: 0.9285714285714286\n",
            "Batch 1 - Loss: 0.09965863500283137, Accuracy: 0.9393939393939394\n",
            "Batch 2 - Loss: 0.11668684793950493, Accuracy: 0.9242424242424242\n",
            "Batch 3 - Loss: 0.20315126648129644, Accuracy: 0.9285714285714286\n",
            "Batch 1 - Loss: 0.10036402930560206, Accuracy: 0.9545454545454546\n",
            "Batch 2 - Loss: 0.1162678547129526, Accuracy: 0.9242424242424242\n",
            "Batch 3 - Loss: 0.20376139306435026, Accuracy: 0.9285714285714286\n",
            "Batch 1 - Loss: 0.1007036906091272, Accuracy: 0.9545454545454546\n",
            "Batch 2 - Loss: 0.11585250933307344, Accuracy: 0.9242424242424242\n",
            "Batch 3 - Loss: 0.20425369019752854, Accuracy: 0.9285714285714286\n",
            "Batch 1 - Loss: 0.10094598010521107, Accuracy: 0.9545454545454546\n",
            "Batch 2 - Loss: 0.115555856128133, Accuracy: 0.9242424242424242\n",
            "Batch 3 - Loss: 0.20458850960061342, Accuracy: 0.9285714285714286\n",
            "Batch 1 - Loss: 0.10110980194001333, Accuracy: 0.9545454545454546\n",
            "Batch 2 - Loss: 0.11533617251482131, Accuracy: 0.9242424242424242\n",
            "Batch 3 - Loss: 0.2048141945695176, Accuracy: 0.9285714285714286\n",
            "Batch 1 - Loss: 0.10121950071301261, Accuracy: 0.9545454545454546\n",
            "Batch 2 - Loss: 0.1151689453711702, Accuracy: 0.9242424242424242\n",
            "Batch 3 - Loss: 0.20496444540727204, Accuracy: 0.9285714285714286\n",
            "Batch 1 - Loss: 0.1012922496577971, Accuracy: 0.9545454545454546\n",
            "Batch 2 - Loss: 0.11503679305030758, Accuracy: 0.9242424242424242\n",
            "Batch 3 - Loss: 0.20506354913556304, Accuracy: 0.9285714285714286\n",
            "Batch 1 - Loss: 0.10134000505035881, Accuracy: 0.9545454545454546\n",
            "Batch 2 - Loss: 0.11492788169285267, Accuracy: 0.9242424242424242\n",
            "Batch 3 - Loss: 0.20512898959719905, Accuracy: 0.9285714285714286\n",
            "Batch 1 - Loss: 0.12059985207473296, Accuracy: 0.9473684210526315\n",
            "Batch 2 - Loss: 0.10533552961033492, Accuracy: 0.9342105263157895\n",
            "Batch 3 - Loss: 0.1893410221550596, Accuracy: 0.875\n",
            "Batch 1 - Loss: 0.2063458947221978, Accuracy: 0.9342105263157895\n",
            "Batch 2 - Loss: 0.09856445200660406, Accuracy: 0.9605263157894737\n",
            "Batch 3 - Loss: 0.19276402327381234, Accuracy: 0.875\n",
            "Batch 1 - Loss: 0.18908887255364756, Accuracy: 0.9473684210526315\n",
            "Batch 2 - Loss: 0.13227553670031514, Accuracy: 0.9210526315789473\n",
            "Batch 3 - Loss: 0.15982863377656717, Accuracy: 0.875\n",
            "Batch 1 - Loss: 0.16800374251974476, Accuracy: 0.9473684210526315\n",
            "Batch 2 - Loss: 0.14511266267352307, Accuracy: 0.9342105263157895\n",
            "Batch 3 - Loss: 0.14654154415171367, Accuracy: 0.875\n",
            "Batch 1 - Loss: 0.16098267546643275, Accuracy: 0.9473684210526315\n",
            "Batch 2 - Loss: 0.14479593902408824, Accuracy: 0.9342105263157895\n",
            "Batch 3 - Loss: 0.14499498760329468, Accuracy: 0.875\n",
            "Batch 1 - Loss: 0.16117912959136027, Accuracy: 0.9473684210526315\n",
            "Batch 2 - Loss: 0.1448434167534752, Accuracy: 0.9342105263157895\n",
            "Batch 3 - Loss: 0.1435009205246432, Accuracy: 0.875\n",
            "Batch 1 - Loss: 0.16100801463136585, Accuracy: 0.9473684210526315\n",
            "Batch 2 - Loss: 0.14492758870453204, Accuracy: 0.9342105263157895\n",
            "Batch 3 - Loss: 0.1420698434611706, Accuracy: 0.875\n",
            "Batch 1 - Loss: 0.16039572273667888, Accuracy: 0.9473684210526315\n",
            "Batch 2 - Loss: 0.14462404185258654, Accuracy: 0.9342105263157895\n",
            "Batch 3 - Loss: 0.1410578652807815, Accuracy: 0.875\n",
            "Batch 1 - Loss: 0.16044559931205735, Accuracy: 0.9473684210526315\n",
            "Batch 2 - Loss: 0.1445953243104631, Accuracy: 0.9342105263157895\n",
            "Batch 3 - Loss: 0.13991858521297665, Accuracy: 0.875\n",
            "Batch 1 - Loss: 0.14490538961864474, Accuracy: 0.9428571428571428\n",
            "Batch 1 - Loss: 0.09211657261126764, Accuracy: 0.9428571428571428\n",
            "Batch 2 - Loss: 0.223576652409064, Accuracy: 0.8857142857142857\n",
            "Batch 2 - Loss: 0.03935369892316587, Accuracy: 1.0\n",
            "Batch 3 - Loss: 0.33757584096998317, Accuracy: 0.9\n",
            "Batch 1 - Loss: 0.2138747286818396, Accuracy: 0.9428571428571428\n",
            "Batch 1 - Loss: 0.11351603668158305, Accuracy: 0.9714285714285714\n",
            "Batch 2 - Loss: 0.2235161414599729, Accuracy: 0.8857142857142857\n",
            "Batch 2 - Loss: 0.03820128446584842, Accuracy: 1.0\n",
            "Batch 3 - Loss: 0.34149760949555796, Accuracy: 0.9\n",
            "Batch 1 - Loss: 0.2028077211598953, Accuracy: 0.9428571428571428\n",
            "Batch 1 - Loss: 0.10272774315544324, Accuracy: 0.9714285714285714\n",
            "Batch 2 - Loss: 0.22095263451688588, Accuracy: 0.8857142857142857\n",
            "Batch 2 - Loss: 0.03860944265931586, Accuracy: 1.0\n",
            "Batch 3 - Loss: 0.33885375859779565, Accuracy: 0.9\n",
            "Batch 1 - Loss: 0.199439563041316, Accuracy: 0.9428571428571428\n",
            "Batch 1 - Loss: 0.1013224533038655, Accuracy: 0.9714285714285714\n",
            "Batch 2 - Loss: 0.21871450808873583, Accuracy: 0.8857142857142857\n",
            "Batch 2 - Loss: 0.03895305934588345, Accuracy: 1.0\n",
            "Batch 3 - Loss: 0.336498470690101, Accuracy: 0.9\n",
            "Batch 1 - Loss: 0.19703658276008915, Accuracy: 0.9428571428571428\n",
            "Batch 1 - Loss: 0.10032820228914233, Accuracy: 0.9714285714285714\n",
            "Batch 2 - Loss: 0.2171847498622282, Accuracy: 0.8857142857142857\n",
            "Batch 2 - Loss: 0.039207251108771386, Accuracy: 1.0\n",
            "Batch 3 - Loss: 0.3346411295450526, Accuracy: 0.9\n",
            "Batch 1 - Loss: 0.195013826161416, Accuracy: 0.9428571428571428\n",
            "Batch 1 - Loss: 0.09939519356011349, Accuracy: 0.9714285714285714\n",
            "Batch 2 - Loss: 0.2160938498900335, Accuracy: 0.8857142857142857\n",
            "Batch 2 - Loss: 0.039398959223450644, Accuracy: 1.0\n",
            "Batch 3 - Loss: 0.333104816711948, Accuracy: 0.9\n",
            "Batch 1 - Loss: 0.19331065906226785, Accuracy: 0.9428571428571428\n",
            "Batch 1 - Loss: 0.0985564214594879, Accuracy: 0.9714285714285714\n",
            "Batch 2 - Loss: 0.21529050057540022, Accuracy: 0.8857142857142857\n",
            "Batch 2 - Loss: 0.03957737755642678, Accuracy: 1.0\n",
            "Batch 3 - Loss: 0.3327292986312822, Accuracy: 0.9\n",
            "Batch 1 - Loss: 0.19203966866011188, Accuracy: 0.9428571428571428\n",
            "Batch 1 - Loss: 0.0980052913030684, Accuracy: 0.9714285714285714\n",
            "Batch 2 - Loss: 0.2145432197644105, Accuracy: 0.8857142857142857\n",
            "Batch 2 - Loss: 0.039711653838318726, Accuracy: 1.0\n",
            "Batch 3 - Loss: 0.3306652196398657, Accuracy: 0.9\n",
            "Batch 1 - Loss: 0.1897768620620445, Accuracy: 0.9428571428571428\n",
            "Batch 1 - Loss: 0.09714399504185273, Accuracy: 0.9714285714285714\n",
            "Batch 2 - Loss: 0.21370970979609014, Accuracy: 0.8857142857142857\n",
            "Batch 2 - Loss: 0.040037290987371535, Accuracy: 1.0\n",
            "Batch 3 - Loss: 0.33228736035761364, Accuracy: 0.9\n",
            "Batch 1 - Loss: 0.20970664650430224, Accuracy: 0.9302325581395349\n",
            "Batch 2 - Loss: 0.12035621469293481, Accuracy: 0.9459459459459459\n",
            "Batch 1 - Loss: 0.09621396688710938, Accuracy: 0.9651162790697675\n",
            "Batch 2 - Loss: 0.1020368003179497, Accuracy: 0.9459459459459459\n",
            "Batch 1 - Loss: 0.09614402066136714, Accuracy: 0.9651162790697675\n",
            "Batch 2 - Loss: 0.09935577887830822, Accuracy: 0.9459459459459459\n",
            "Batch 1 - Loss: 0.09585447359305774, Accuracy: 0.9651162790697675\n",
            "Batch 2 - Loss: 0.09771084930929538, Accuracy: 0.9459459459459459\n",
            "Batch 1 - Loss: 0.09560225781945356, Accuracy: 0.9651162790697675\n",
            "Batch 2 - Loss: 0.09639723652145529, Accuracy: 0.9459459459459459\n",
            "Batch 1 - Loss: 0.0953742105756941, Accuracy: 0.9651162790697675\n",
            "Batch 2 - Loss: 0.09528685212926927, Accuracy: 0.9459459459459459\n",
            "Batch 1 - Loss: 0.0951749737294224, Accuracy: 0.9651162790697675\n",
            "Batch 2 - Loss: 0.09432080522348393, Accuracy: 0.9459459459459459\n",
            "Batch 1 - Loss: 0.09500911867062012, Accuracy: 0.9651162790697675\n",
            "Batch 2 - Loss: 0.09358048061029822, Accuracy: 0.9459459459459459\n",
            "Batch 1 - Loss: 0.0949024504093094, Accuracy: 0.9651162790697675\n",
            "Batch 2 - Loss: 0.09289680148975518, Accuracy: 0.9459459459459459\n",
            "Batch 1 - Loss: 0.09359877500920133, Accuracy: 0.9655172413793104\n",
            "Batch 2 - Loss: 0.09373678326285885, Accuracy: 0.9452054794520548\n",
            "Batch 1 - Loss: 0.09347908383435154, Accuracy: 0.9655172413793104\n",
            "Batch 2 - Loss: 0.09323308521424588, Accuracy: 0.9452054794520548\n",
            "Batch 1 - Loss: 0.09335186949631526, Accuracy: 0.9655172413793104\n",
            "Batch 2 - Loss: 0.09289714235753625, Accuracy: 0.9452054794520548\n",
            "Batch 1 - Loss: 0.09324329280809646, Accuracy: 0.9655172413793104\n",
            "Batch 2 - Loss: 0.0924541738709585, Accuracy: 0.9452054794520548\n",
            "Batch 1 - Loss: 0.09311038285338076, Accuracy: 0.9655172413793104\n",
            "Batch 2 - Loss: 0.09218748722779303, Accuracy: 0.9452054794520548\n",
            "Batch 1 - Loss: 0.09300572695443937, Accuracy: 0.9655172413793104\n",
            "Batch 2 - Loss: 0.09182046876894523, Accuracy: 0.9452054794520548\n",
            "Batch 1 - Loss: 0.09287263288554519, Accuracy: 0.9655172413793104\n",
            "Batch 2 - Loss: 0.09157399309664872, Accuracy: 0.9452054794520548\n",
            "Batch 1 - Loss: 0.09276425675723844, Accuracy: 0.9655172413793104\n",
            "Batch 2 - Loss: 0.09125745024734118, Accuracy: 0.9452054794520548\n",
            "Batch 1 - Loss: 0.09265043917474881, Accuracy: 0.9655172413793104\n",
            "Batch 2 - Loss: 0.09107157520335137, Accuracy: 0.9452054794520548\n",
            "Batch 1 - Loss: 0.11272596626957332, Accuracy: 0.9534883720930233\n",
            "Batch 1 - Loss: 0.10307574927835415, Accuracy: 0.9534883720930233\n",
            "Batch 2 - Loss: 0.03967144304334716, Accuracy: 1.0\n",
            "Batch 3 - Loss: 0.23264986340733917, Accuracy: 0.9354838709677419\n",
            "Batch 1 - Loss: 0.11379356797787388, Accuracy: 0.9534883720930233\n",
            "Batch 1 - Loss: 0.1145128781196383, Accuracy: 0.9534883720930233\n",
            "Batch 2 - Loss: 0.044983051405069946, Accuracy: 0.9767441860465116\n",
            "Batch 3 - Loss: 0.23133384188409514, Accuracy: 0.9354838709677419\n",
            "Batch 1 - Loss: 0.10942054966355512, Accuracy: 0.9534883720930233\n",
            "Batch 1 - Loss: 0.11841789710308141, Accuracy: 0.9534883720930233\n",
            "Batch 2 - Loss: 0.04876489937823876, Accuracy: 0.9767441860465116\n",
            "Batch 3 - Loss: 0.23169133401208925, Accuracy: 0.9354838709677419\n",
            "Batch 1 - Loss: 0.10750038719314574, Accuracy: 0.9534883720930233\n",
            "Batch 1 - Loss: 0.1192868586845326, Accuracy: 0.9534883720930233\n",
            "Batch 2 - Loss: 0.05067424085805772, Accuracy: 0.9767441860465116\n",
            "Batch 3 - Loss: 0.23003211799844026, Accuracy: 0.9354838709677419\n",
            "Batch 1 - Loss: 0.106444808438051, Accuracy: 0.9534883720930233\n",
            "Batch 1 - Loss: 0.11940283985761546, Accuracy: 0.9534883720930233\n",
            "Batch 2 - Loss: 0.05185430779075259, Accuracy: 0.9767441860465116\n",
            "Batch 3 - Loss: 0.23083189674077634, Accuracy: 0.9354838709677419\n",
            "Batch 1 - Loss: 0.10585139109594179, Accuracy: 0.9534883720930233\n",
            "Batch 1 - Loss: 0.11934009144623625, Accuracy: 0.9534883720930233\n",
            "Batch 2 - Loss: 0.052819650997560724, Accuracy: 0.9767441860465116\n",
            "Batch 3 - Loss: 0.22945664525937542, Accuracy: 0.9354838709677419\n",
            "Batch 1 - Loss: 0.1053719537660732, Accuracy: 0.9534883720930233\n",
            "Batch 1 - Loss: 0.11913851118372049, Accuracy: 0.9534883720930233\n",
            "Batch 2 - Loss: 0.053284424069527755, Accuracy: 0.9767441860465116\n",
            "Batch 3 - Loss: 0.22988490272040452, Accuracy: 0.9354838709677419\n",
            "Batch 1 - Loss: 0.10525771646384126, Accuracy: 0.9534883720930233\n",
            "Batch 1 - Loss: 0.11902033213987467, Accuracy: 0.9534883720930233\n",
            "Batch 2 - Loss: 0.05352417413639774, Accuracy: 0.9767441860465116\n",
            "Batch 3 - Loss: 0.22925986744593238, Accuracy: 0.9354838709677419\n",
            "Batch 1 - Loss: 0.10498024125986025, Accuracy: 0.9534883720930233\n",
            "Batch 1 - Loss: 0.11883547471828433, Accuracy: 0.9534883720930233\n",
            "Batch 2 - Loss: 0.05383652105608185, Accuracy: 0.9767441860465116\n",
            "Batch 3 - Loss: 0.23019200992608965, Accuracy: 0.9354838709677419\n",
            "Batch 1 - Loss: 0.11499665097033715, Accuracy: 0.9574468085106383\n",
            "Batch 1 - Loss: 0.09212376280804603, Accuracy: 0.9574468085106383\n",
            "Batch 2 - Loss: 0.08023682603020174, Accuracy: 0.9574468085106383\n",
            "Batch 3 - Loss: 0.38360810090085956, Accuracy: 0.8947368421052632\n",
            "Batch 1 - Loss: 0.27487190832236036, Accuracy: 0.8723404255319149\n",
            "Batch 1 - Loss: 0.03702425585672656, Accuracy: 0.9787234042553191\n",
            "Batch 2 - Loss: 0.04714361346768358, Accuracy: 0.9787234042553191\n",
            "Batch 3 - Loss: 0.5404414864454413, Accuracy: 0.8947368421052632\n",
            "Batch 1 - Loss: 0.20230899614843662, Accuracy: 0.9148936170212766\n",
            "Batch 1 - Loss: 0.12723567951849152, Accuracy: 0.9361702127659575\n",
            "Batch 2 - Loss: 0.14302448834096854, Accuracy: 0.9361702127659575\n",
            "Batch 3 - Loss: 0.3640585147713486, Accuracy: 0.8947368421052632\n",
            "Batch 1 - Loss: 0.19470178861120702, Accuracy: 0.9361702127659575\n",
            "Batch 1 - Loss: 0.1044368071650118, Accuracy: 0.9361702127659575\n",
            "Batch 2 - Loss: 0.11465237545928797, Accuracy: 0.9574468085106383\n",
            "Batch 3 - Loss: 0.38141999334840926, Accuracy: 0.8947368421052632\n",
            "Batch 1 - Loss: 0.19783627551594485, Accuracy: 0.9361702127659575\n",
            "Batch 1 - Loss: 0.09561340646136919, Accuracy: 0.9361702127659575\n",
            "Batch 2 - Loss: 0.10584673656561641, Accuracy: 0.9574468085106383\n",
            "Batch 3 - Loss: 0.38933348169876697, Accuracy: 0.8947368421052632\n",
            "Batch 1 - Loss: 0.19582482094290174, Accuracy: 0.9361702127659575\n",
            "Batch 1 - Loss: 0.09555197050773835, Accuracy: 0.9361702127659575\n",
            "Batch 2 - Loss: 0.10660577014716566, Accuracy: 0.9574468085106383\n",
            "Batch 3 - Loss: 0.384084580428205, Accuracy: 0.8947368421052632\n",
            "Batch 1 - Loss: 0.19252634468973676, Accuracy: 0.9361702127659575\n",
            "Batch 1 - Loss: 0.09648355354723868, Accuracy: 0.9361702127659575\n",
            "Batch 2 - Loss: 0.10815074318220108, Accuracy: 0.9574468085106383\n",
            "Batch 3 - Loss: 0.37723665760200986, Accuracy: 0.8947368421052632\n",
            "Batch 1 - Loss: 0.1898171425661785, Accuracy: 0.9361702127659575\n",
            "Batch 1 - Loss: 0.09688670058089688, Accuracy: 0.9361702127659575\n",
            "Batch 2 - Loss: 0.10896848580009041, Accuracy: 0.9574468085106383\n",
            "Batch 3 - Loss: 0.37200381318856385, Accuracy: 0.8947368421052632\n",
            "Batch 1 - Loss: 0.18770783377318867, Accuracy: 0.9361702127659575\n",
            "Batch 1 - Loss: 0.09709628340812995, Accuracy: 0.9361702127659575\n",
            "Batch 2 - Loss: 0.10949205139270932, Accuracy: 0.9574468085106383\n",
            "Batch 3 - Loss: 0.367929834101494, Accuracy: 0.8947368421052632\n",
            "Batch 1 - Loss: 0.1831017026956985, Accuracy: 0.9452054794520548\n",
            "Batch 2 - Loss: 0.1635099104604357, Accuracy: 0.9178082191780822\n",
            "Batch 3 - Loss: 0.12150404749278627, Accuracy: 0.9285714285714286\n",
            "Batch 1 - Loss: 0.08670173178241762, Accuracy: 0.9452054794520548\n",
            "Batch 2 - Loss: 0.1573540378057634, Accuracy: 0.9178082191780822\n",
            "Batch 3 - Loss: 0.12031886427062463, Accuracy: 0.9285714285714286\n",
            "Batch 1 - Loss: 0.08753880303356786, Accuracy: 0.9452054794520548\n",
            "Batch 2 - Loss: 0.1527096154001583, Accuracy: 0.9041095890410958\n",
            "Batch 3 - Loss: 0.12119544973974407, Accuracy: 0.9285714285714286\n",
            "Batch 1 - Loss: 0.08783299763833602, Accuracy: 0.9452054794520548\n",
            "Batch 2 - Loss: 0.1486338222874124, Accuracy: 0.9041095890410958\n",
            "Batch 3 - Loss: 0.12228118488869565, Accuracy: 0.9285714285714286\n",
            "Batch 1 - Loss: 0.08781221609954319, Accuracy: 0.9452054794520548\n",
            "Batch 2 - Loss: 0.14522190617470476, Accuracy: 0.9041095890410958\n",
            "Batch 3 - Loss: 0.1234773987339244, Accuracy: 0.9285714285714286\n",
            "Batch 1 - Loss: 0.08871142966970617, Accuracy: 0.9452054794520548\n",
            "Batch 2 - Loss: 0.1439862147100544, Accuracy: 0.9041095890410958\n",
            "Batch 3 - Loss: 0.12387786675260269, Accuracy: 0.9285714285714286\n",
            "Batch 1 - Loss: 0.08930711024554697, Accuracy: 0.9452054794520548\n",
            "Batch 2 - Loss: 0.143019345571111, Accuracy: 0.9041095890410958\n",
            "Batch 3 - Loss: 0.12449460415720749, Accuracy: 0.9285714285714286\n",
            "Batch 1 - Loss: 0.08895249716958462, Accuracy: 0.9452054794520548\n",
            "Batch 2 - Loss: 0.14098941121271377, Accuracy: 0.9041095890410958\n",
            "Batch 3 - Loss: 0.12522773677779162, Accuracy: 0.9285714285714286\n",
            "Batch 1 - Loss: 0.09000427919077751, Accuracy: 0.9452054794520548\n",
            "Batch 2 - Loss: 0.14101656195507623, Accuracy: 0.9041095890410958\n",
            "Batch 3 - Loss: 0.12523628239772427, Accuracy: 0.9285714285714286\n",
            "Batch 1 - Loss: 0.08955332177384646, Accuracy: 0.9452054794520548\n",
            "Batch 2 - Loss: 0.13952821994025172, Accuracy: 0.9041095890410958\n",
            "Batch 3 - Loss: 0.12578058486315874, Accuracy: 0.9285714285714286\n",
            "Batch 1 - Loss: 0.10449352897656328, Accuracy: 0.9393939393939394\n",
            "Batch 1 - Loss: 0.11314149004607402, Accuracy: 0.9696969696969697\n",
            "Batch 2 - Loss: 0.1637763873457905, Accuracy: 0.9090909090909091\n",
            "Batch 2 - Loss: 0.052555707699357586, Accuracy: 0.9696969696969697\n",
            "Batch 3 - Loss: 0.2486546855644535, Accuracy: 0.9285714285714286\n",
            "Batch 1 - Loss: 0.11678249851512856, Accuracy: 0.9393939393939394\n",
            "Batch 1 - Loss: 0.10743070593036182, Accuracy: 0.9696969696969697\n",
            "Batch 2 - Loss: 0.1600088777718186, Accuracy: 0.9090909090909091\n",
            "Batch 2 - Loss: 0.05219749823967342, Accuracy: 0.9696969696969697\n",
            "Batch 3 - Loss: 0.246303624181825, Accuracy: 0.9285714285714286\n",
            "Batch 1 - Loss: 0.11565273776756717, Accuracy: 0.9393939393939394\n",
            "Batch 1 - Loss: 0.10798408412803236, Accuracy: 0.9696969696969697\n",
            "Batch 2 - Loss: 0.1593541604163437, Accuracy: 0.9090909090909091\n",
            "Batch 2 - Loss: 0.052594910959667264, Accuracy: 0.9696969696969697\n",
            "Batch 3 - Loss: 0.24484191148924775, Accuracy: 0.9285714285714286\n",
            "Batch 1 - Loss: 0.11476400296212681, Accuracy: 0.9393939393939394\n",
            "Batch 1 - Loss: 0.10829269863362585, Accuracy: 0.9696969696969697\n",
            "Batch 2 - Loss: 0.1588851454173103, Accuracy: 0.9090909090909091\n",
            "Batch 2 - Loss: 0.05294195830233551, Accuracy: 0.9696969696969697\n",
            "Batch 3 - Loss: 0.24396302896246613, Accuracy: 0.9285714285714286\n",
            "Batch 1 - Loss: 0.1140574361759783, Accuracy: 0.9393939393939394\n",
            "Batch 1 - Loss: 0.10849576382245603, Accuracy: 0.9696969696969697\n",
            "Batch 2 - Loss: 0.15850847614041194, Accuracy: 0.9090909090909091\n",
            "Batch 2 - Loss: 0.05324606257119569, Accuracy: 0.9696969696969697\n",
            "Batch 3 - Loss: 0.24324881206972895, Accuracy: 0.9285714285714286\n",
            "Batch 1 - Loss: 0.11346119697654643, Accuracy: 0.9393939393939394\n",
            "Batch 1 - Loss: 0.10864465105448502, Accuracy: 0.9696969696969697\n",
            "Batch 2 - Loss: 0.15820409806438435, Accuracy: 0.9090909090909091\n",
            "Batch 2 - Loss: 0.05350352061125885, Accuracy: 0.9696969696969697\n",
            "Batch 3 - Loss: 0.24265969216900915, Accuracy: 0.9285714285714286\n",
            "Batch 1 - Loss: 0.1129590907510563, Accuracy: 0.9393939393939394\n",
            "Batch 1 - Loss: 0.10875781456495323, Accuracy: 0.9696969696969697\n",
            "Batch 2 - Loss: 0.15795514976537403, Accuracy: 0.9090909090909091\n",
            "Batch 2 - Loss: 0.053716193277540916, Accuracy: 0.9696969696969697\n",
            "Batch 3 - Loss: 0.24216898961794123, Accuracy: 0.9285714285714286\n",
            "Batch 1 - Loss: 0.11253621037522285, Accuracy: 0.9393939393939394\n",
            "Batch 1 - Loss: 0.10884612495525894, Accuracy: 0.9696969696969697\n",
            "Batch 2 - Loss: 0.15774957207468088, Accuracy: 0.9090909090909091\n",
            "Batch 2 - Loss: 0.053891584565665736, Accuracy: 0.9696969696969697\n",
            "Batch 3 - Loss: 0.2421059635906809, Accuracy: 0.9285714285714286\n",
            "Batch 1 - Loss: 0.1122067441390736, Accuracy: 0.9393939393939394\n",
            "Batch 1 - Loss: 0.10894179236055511, Accuracy: 0.9696969696969697\n",
            "Batch 2 - Loss: 0.15751863707773378, Accuracy: 0.9090909090909091\n",
            "Batch 2 - Loss: 0.05406590020837215, Accuracy: 0.9696969696969697\n",
            "Batch 3 - Loss: 0.2413844870795043, Accuracy: 0.9285714285714286\n",
            "Batch 1 - Loss: 0.11184438697907571, Accuracy: 0.9393939393939394\n",
            "Batch 1 - Loss: 0.10900639177723354, Accuracy: 0.9696969696969697\n",
            "Batch 2 - Loss: 0.15738646283299063, Accuracy: 0.9090909090909091\n",
            "Batch 2 - Loss: 0.05417209173790328, Accuracy: 0.9696969696969697\n",
            "Batch 3 - Loss: 0.24109565778274936, Accuracy: 0.9285714285714286\n",
            "Batch 1 - Loss: 0.11741562627761157, Accuracy: 0.9565217391304348\n",
            "Batch 1 - Loss: 0.09356866030765508, Accuracy: 0.9565217391304348\n",
            "Batch 2 - Loss: 0.057928877280349206, Accuracy: 0.9782608695652174\n",
            "Batch 3 - Loss: 0.27536161177998675, Accuracy: 0.9090909090909091\n",
            "Batch 1 - Loss: 0.1358076652872445, Accuracy: 0.9565217391304348\n",
            "Batch 1 - Loss: 0.09228122563975028, Accuracy: 0.9565217391304348\n",
            "Batch 2 - Loss: 0.05760901781158072, Accuracy: 0.9782608695652174\n",
            "Batch 3 - Loss: 0.278881291599973, Accuracy: 0.9090909090909091\n",
            "Batch 1 - Loss: 0.13559203180620005, Accuracy: 0.9565217391304348\n",
            "Batch 1 - Loss: 0.09309635094578426, Accuracy: 0.9565217391304348\n",
            "Batch 2 - Loss: 0.058443624145796874, Accuracy: 0.9782608695652174\n",
            "Batch 3 - Loss: 0.27897899457776115, Accuracy: 0.9090909090909091\n",
            "Batch 1 - Loss: 0.13488902740338374, Accuracy: 0.9565217391304348\n",
            "Batch 1 - Loss: 0.09368423944662856, Accuracy: 0.9565217391304348\n",
            "Batch 2 - Loss: 0.059019002660130646, Accuracy: 0.9782608695652174\n",
            "Batch 3 - Loss: 0.27908927776269005, Accuracy: 0.9090909090909091\n",
            "Batch 1 - Loss: 0.13448253581161226, Accuracy: 0.9565217391304348\n",
            "Batch 1 - Loss: 0.09388786019821457, Accuracy: 0.9565217391304348\n",
            "Batch 2 - Loss: 0.059312488857393444, Accuracy: 0.9782608695652174\n",
            "Batch 3 - Loss: 0.2793630258187474, Accuracy: 0.9090909090909091\n",
            "Batch 1 - Loss: 0.13406770189118147, Accuracy: 0.9565217391304348\n",
            "Batch 1 - Loss: 0.09373342881651583, Accuracy: 0.9565217391304348\n",
            "Batch 2 - Loss: 0.05939141272047939, Accuracy: 0.9782608695652174\n",
            "Batch 3 - Loss: 0.27973847241029465, Accuracy: 0.9090909090909091\n",
            "Batch 1 - Loss: 0.1341836894506536, Accuracy: 0.9565217391304348\n",
            "Batch 1 - Loss: 0.09360730549799777, Accuracy: 0.9565217391304348\n",
            "Batch 2 - Loss: 0.05944729959436187, Accuracy: 0.9782608695652174\n",
            "Batch 3 - Loss: 0.28014793060053583, Accuracy: 0.9090909090909091\n",
            "Batch 1 - Loss: 0.1341167923570614, Accuracy: 0.9565217391304348\n",
            "Batch 1 - Loss: 0.09351247674269224, Accuracy: 0.9565217391304348\n",
            "Batch 2 - Loss: 0.0595037107445832, Accuracy: 0.9782608695652174\n",
            "Batch 3 - Loss: 0.28051098355661747, Accuracy: 0.9090909090909091\n",
            "Batch 1 - Loss: 0.1340761271461642, Accuracy: 0.9565217391304348\n",
            "Batch 1 - Loss: 0.09338148665724184, Accuracy: 0.9565217391304348\n",
            "Batch 2 - Loss: 0.05953106607878945, Accuracy: 0.9782608695652174\n",
            "Batch 3 - Loss: 0.2808732414871241, Accuracy: 0.9090909090909091\n",
            "Batch 1 - Loss: 0.13405817484871826, Accuracy: 0.9565217391304348\n",
            "Batch 1 - Loss: 0.09323167302193122, Accuracy: 0.9565217391304348\n",
            "Batch 2 - Loss: 0.05954257608001165, Accuracy: 0.9782608695652174\n",
            "Batch 3 - Loss: 0.2812293475851634, Accuracy: 0.9090909090909091\n",
            "Batch 1 - Loss: 0.12393322814292862, Accuracy: 0.9545454545454546\n",
            "Batch 2 - Loss: 0.1199169006791367, Accuracy: 0.9242424242424242\n",
            "Batch 3 - Loss: 0.20009138804323254, Accuracy: 0.9285714285714286\n",
            "Batch 1 - Loss: 0.09622721631084288, Accuracy: 0.9393939393939394\n",
            "Batch 2 - Loss: 0.11631122929562632, Accuracy: 0.9242424242424242\n",
            "Batch 3 - Loss: 0.20261733726391856, Accuracy: 0.9285714285714286\n",
            "Batch 1 - Loss: 0.09703770492389184, Accuracy: 0.9545454545454546\n",
            "Batch 2 - Loss: 0.11594448750079474, Accuracy: 0.9242424242424242\n",
            "Batch 3 - Loss: 0.2032123748277008, Accuracy: 0.9285714285714286\n",
            "Batch 1 - Loss: 0.0973846937154115, Accuracy: 0.9545454545454546\n",
            "Batch 2 - Loss: 0.1155231333653224, Accuracy: 0.9242424242424242\n",
            "Batch 3 - Loss: 0.20371798680501638, Accuracy: 0.9285714285714286\n",
            "Batch 1 - Loss: 0.0976384748677123, Accuracy: 0.9545454545454546\n",
            "Batch 2 - Loss: 0.11522554773322134, Accuracy: 0.9242424242424242\n",
            "Batch 3 - Loss: 0.20406176469877532, Accuracy: 0.9285714285714286\n",
            "Batch 1 - Loss: 0.09781397088013875, Accuracy: 0.9545454545454546\n",
            "Batch 2 - Loss: 0.11500581874001899, Accuracy: 0.9242424242424242\n",
            "Batch 3 - Loss: 0.2042950679399766, Accuracy: 0.9285714285714286\n",
            "Batch 1 - Loss: 0.0979356620095724, Accuracy: 0.9545454545454546\n",
            "Batch 2 - Loss: 0.11483907594795555, Accuracy: 0.9242424242424242\n",
            "Batch 3 - Loss: 0.2044520435618556, Accuracy: 0.9285714285714286\n",
            "Batch 1 - Loss: 0.09802052643847407, Accuracy: 0.9545454545454546\n",
            "Batch 2 - Loss: 0.11470780572511596, Accuracy: 0.9242424242424242\n",
            "Batch 3 - Loss: 0.2045571564183393, Accuracy: 0.9285714285714286\n",
            "Batch 1 - Loss: 0.09808036745419876, Accuracy: 0.9545454545454546\n",
            "Batch 2 - Loss: 0.1146001288594835, Accuracy: 0.9242424242424242\n",
            "Batch 3 - Loss: 0.20462728856657783, Accuracy: 0.9285714285714286\n",
            "Batch 1 - Loss: 0.09812321667156569, Accuracy: 0.9545454545454546\n",
            "Batch 2 - Loss: 0.11450809692398006, Accuracy: 0.9242424242424242\n",
            "Batch 3 - Loss: 0.20467388711305642, Accuracy: 0.9285714285714286\n",
            "Batch 1 - Loss: 0.11789884373462801, Accuracy: 0.9473684210526315\n",
            "Batch 2 - Loss: 0.10492104801827376, Accuracy: 0.9342105263157895\n",
            "Batch 3 - Loss: 0.18875226841272796, Accuracy: 0.875\n",
            "Batch 1 - Loss: 0.20276936932102085, Accuracy: 0.9473684210526315\n",
            "Batch 2 - Loss: 0.0977955286410937, Accuracy: 0.9605263157894737\n",
            "Batch 3 - Loss: 0.19209690282753936, Accuracy: 0.875\n",
            "Batch 1 - Loss: 0.18557778930600652, Accuracy: 0.9473684210526315\n",
            "Batch 2 - Loss: 0.13121482370155998, Accuracy: 0.9210526315789473\n",
            "Batch 3 - Loss: 0.15923434454242746, Accuracy: 0.875\n",
            "Batch 1 - Loss: 0.1644412934480138, Accuracy: 0.9473684210526315\n",
            "Batch 2 - Loss: 0.1448263211382004, Accuracy: 0.9342105263157895\n",
            "Batch 3 - Loss: 0.14534580003744305, Accuracy: 0.875\n",
            "Batch 1 - Loss: 0.1572418880856671, Accuracy: 0.9473684210526315\n",
            "Batch 2 - Loss: 0.14482050669594007, Accuracy: 0.9342105263157895\n",
            "Batch 3 - Loss: 0.14357799214739086, Accuracy: 0.875\n",
            "Batch 1 - Loss: 0.15736908140197853, Accuracy: 0.9473684210526315\n",
            "Batch 2 - Loss: 0.14496623178122733, Accuracy: 0.9342105263157895\n",
            "Batch 3 - Loss: 0.14201545951583483, Accuracy: 0.875\n",
            "Batch 1 - Loss: 0.15714848645839666, Accuracy: 0.9473684210526315\n",
            "Batch 2 - Loss: 0.1450563084657997, Accuracy: 0.9342105263157895\n",
            "Batch 3 - Loss: 0.14060203794295184, Accuracy: 0.875\n",
            "Batch 1 - Loss: 0.15661431500371556, Accuracy: 0.9473684210526315\n",
            "Batch 2 - Loss: 0.14478456479280175, Accuracy: 0.9342105263157895\n",
            "Batch 3 - Loss: 0.13960690453463426, Accuracy: 0.875\n",
            "Batch 1 - Loss: 0.15544122011904027, Accuracy: 0.9473684210526315\n",
            "Batch 2 - Loss: 0.14384609353667285, Accuracy: 0.9342105263157895\n",
            "Batch 3 - Loss: 0.13910181292308513, Accuracy: 0.875\n",
            "Batch 1 - Loss: 0.15624058410919064, Accuracy: 0.9473684210526315\n",
            "Batch 2 - Loss: 0.14413165605738748, Accuracy: 0.9342105263157895\n",
            "Batch 3 - Loss: 0.13781520334664374, Accuracy: 0.875\n",
            "Batch 1 - Loss: 0.14445347466127115, Accuracy: 0.9428571428571428\n",
            "Batch 1 - Loss: 0.08726004494944954, Accuracy: 0.9428571428571428\n",
            "Batch 2 - Loss: 0.22530591146668308, Accuracy: 0.8857142857142857\n",
            "Batch 2 - Loss: 0.03927989777634868, Accuracy: 1.0\n",
            "Batch 3 - Loss: 0.33626015271421295, Accuracy: 0.9\n",
            "Batch 1 - Loss: 0.21350112839499144, Accuracy: 0.9428571428571428\n",
            "Batch 1 - Loss: 0.10936257293880688, Accuracy: 0.9714285714285714\n",
            "Batch 2 - Loss: 0.22463075776166141, Accuracy: 0.8857142857142857\n",
            "Batch 2 - Loss: 0.03804996870801832, Accuracy: 1.0\n",
            "Batch 3 - Loss: 0.3403220785099629, Accuracy: 0.9\n",
            "Batch 1 - Loss: 0.20313448737588496, Accuracy: 0.9428571428571428\n",
            "Batch 1 - Loss: 0.09878151429741545, Accuracy: 0.9714285714285714\n",
            "Batch 2 - Loss: 0.2220886203606257, Accuracy: 0.8857142857142857\n",
            "Batch 2 - Loss: 0.03837928753167265, Accuracy: 1.0\n",
            "Batch 3 - Loss: 0.3381848262996734, Accuracy: 0.9\n",
            "Batch 1 - Loss: 0.1994484505357572, Accuracy: 0.9428571428571428\n",
            "Batch 1 - Loss: 0.09679101595491879, Accuracy: 0.9714285714285714\n",
            "Batch 2 - Loss: 0.2201854331875821, Accuracy: 0.8857142857142857\n",
            "Batch 2 - Loss: 0.038755136160157876, Accuracy: 1.0\n",
            "Batch 3 - Loss: 0.33579301626981495, Accuracy: 0.9\n",
            "Batch 1 - Loss: 0.19677421687388244, Accuracy: 0.9428571428571428\n",
            "Batch 1 - Loss: 0.09568648141640106, Accuracy: 0.9714285714285714\n",
            "Batch 2 - Loss: 0.21847936879239602, Accuracy: 0.8857142857142857\n",
            "Batch 2 - Loss: 0.03904779004721635, Accuracy: 1.0\n",
            "Batch 3 - Loss: 0.3337874422102117, Accuracy: 0.9\n",
            "Batch 1 - Loss: 0.19469841165967122, Accuracy: 0.9428571428571428\n",
            "Batch 1 - Loss: 0.09480517415989506, Accuracy: 0.9714285714285714\n",
            "Batch 2 - Loss: 0.21720371627615748, Accuracy: 0.8857142857142857\n",
            "Batch 2 - Loss: 0.03926506181624956, Accuracy: 1.0\n",
            "Batch 3 - Loss: 0.3321665131282268, Accuracy: 0.9\n",
            "Batch 1 - Loss: 0.19295202039349127, Accuracy: 0.9428571428571428\n",
            "Batch 1 - Loss: 0.09398349990962492, Accuracy: 0.9714285714285714\n",
            "Batch 2 - Loss: 0.21627259135471202, Accuracy: 0.8857142857142857\n",
            "Batch 2 - Loss: 0.039425550826790154, Accuracy: 1.0\n",
            "Batch 3 - Loss: 0.33082833480779, Accuracy: 0.9\n",
            "Batch 1 - Loss: 0.19145833781538454, Accuracy: 0.9428571428571428\n",
            "Batch 1 - Loss: 0.09323478545059706, Accuracy: 0.9714285714285714\n",
            "Batch 2 - Loss: 0.2155705832081036, Accuracy: 0.8857142857142857\n",
            "Batch 2 - Loss: 0.03957420391152196, Accuracy: 1.0\n",
            "Batch 3 - Loss: 0.3304957762918047, Accuracy: 0.9\n",
            "Batch 1 - Loss: 0.19032726551157608, Accuracy: 0.9428571428571428\n",
            "Batch 1 - Loss: 0.09273845919147845, Accuracy: 0.9714285714285714\n",
            "Batch 2 - Loss: 0.21490011408188225, Accuracy: 0.8857142857142857\n",
            "Batch 2 - Loss: 0.039831738021348075, Accuracy: 1.0\n",
            "Batch 3 - Loss: 0.3316173260594241, Accuracy: 0.9\n",
            "Batch 1 - Loss: 0.1886642163437238, Accuracy: 0.9428571428571428\n",
            "Batch 1 - Loss: 0.09249905661595101, Accuracy: 0.9714285714285714\n",
            "Batch 2 - Loss: 0.2135644869955465, Accuracy: 0.8857142857142857\n",
            "Batch 2 - Loss: 0.04021117646412295, Accuracy: 1.0\n",
            "Batch 3 - Loss: 0.3304123405512131, Accuracy: 0.9\n",
            "Batch 1 - Loss: 0.20593971470463748, Accuracy: 0.9302325581395349\n",
            "Batch 2 - Loss: 0.11971384877103251, Accuracy: 0.9459459459459459\n",
            "Batch 1 - Loss: 0.09456745293249066, Accuracy: 0.9651162790697675\n",
            "Batch 2 - Loss: 0.10172027114935293, Accuracy: 0.9459459459459459\n",
            "Batch 1 - Loss: 0.09444447788751913, Accuracy: 0.9651162790697675\n",
            "Batch 2 - Loss: 0.0989815366998773, Accuracy: 0.9459459459459459\n",
            "Batch 1 - Loss: 0.09409641985673856, Accuracy: 0.9651162790697675\n",
            "Batch 2 - Loss: 0.09732549354099242, Accuracy: 0.9459459459459459\n",
            "Batch 1 - Loss: 0.0938069541988281, Accuracy: 0.9651162790697675\n",
            "Batch 2 - Loss: 0.09599981599060838, Accuracy: 0.9459459459459459\n",
            "Batch 1 - Loss: 0.09355689526568402, Accuracy: 0.9651162790697675\n",
            "Batch 2 - Loss: 0.09488026947592225, Accuracy: 0.9459459459459459\n",
            "Batch 1 - Loss: 0.0933962997451065, Accuracy: 0.9651162790697675\n",
            "Batch 2 - Loss: 0.09399661192825826, Accuracy: 0.9459459459459459\n",
            "Batch 1 - Loss: 0.09323280140373223, Accuracy: 0.9651162790697675\n",
            "Batch 2 - Loss: 0.09312626313968358, Accuracy: 0.9459459459459459\n",
            "Batch 1 - Loss: 0.09310466287950195, Accuracy: 0.9651162790697675\n",
            "Batch 2 - Loss: 0.09252694530898523, Accuracy: 0.9459459459459459\n",
            "Batch 1 - Loss: 0.09288471075537838, Accuracy: 0.9651162790697675\n",
            "Batch 2 - Loss: 0.091974575081051, Accuracy: 0.9459459459459459\n",
            "Batch 1 - Loss: 0.09171336741521904, Accuracy: 0.9655172413793104\n",
            "Batch 2 - Loss: 0.09285985496475364, Accuracy: 0.9452054794520548\n",
            "Batch 1 - Loss: 0.09158727103797681, Accuracy: 0.9655172413793104\n",
            "Batch 2 - Loss: 0.09238317064470748, Accuracy: 0.9452054794520548\n",
            "Batch 1 - Loss: 0.09148716032114022, Accuracy: 0.9655172413793104\n",
            "Batch 2 - Loss: 0.09205273238362285, Accuracy: 0.9452054794520548\n",
            "Batch 1 - Loss: 0.0913656874995238, Accuracy: 0.9655172413793104\n",
            "Batch 2 - Loss: 0.09166397990625602, Accuracy: 0.9452054794520548\n",
            "Batch 1 - Loss: 0.09127325628647476, Accuracy: 0.9655172413793104\n",
            "Batch 2 - Loss: 0.09140604142478745, Accuracy: 0.9452054794520548\n",
            "Batch 1 - Loss: 0.09115051292938073, Accuracy: 0.9655172413793104\n",
            "Batch 2 - Loss: 0.09109826931257979, Accuracy: 0.9452054794520548\n",
            "Batch 1 - Loss: 0.09106290462028876, Accuracy: 0.9655172413793104\n",
            "Batch 2 - Loss: 0.09077704599475103, Accuracy: 0.9452054794520548\n",
            "Batch 1 - Loss: 0.09094830130891257, Accuracy: 0.9655172413793104\n",
            "Batch 2 - Loss: 0.09058755048957615, Accuracy: 0.9452054794520548\n",
            "Batch 1 - Loss: 0.09087254201761634, Accuracy: 0.9655172413793104\n",
            "Batch 2 - Loss: 0.09028528230432767, Accuracy: 0.9452054794520548\n",
            "Batch 1 - Loss: 0.09076710995953888, Accuracy: 0.9655172413793104\n",
            "Batch 2 - Loss: 0.0901295908332741, Accuracy: 0.9452054794520548\n",
            "Batch 1 - Loss: 0.11278849761634663, Accuracy: 0.9534883720930233\n",
            "Batch 1 - Loss: 0.0994966645193523, Accuracy: 0.9534883720930233\n",
            "Batch 2 - Loss: 0.03812957053314473, Accuracy: 1.0\n",
            "Batch 3 - Loss: 0.23330169035740225, Accuracy: 0.9354838709677419\n",
            "Batch 1 - Loss: 0.11472026197321256, Accuracy: 0.9534883720930233\n",
            "Batch 1 - Loss: 0.10975378806760805, Accuracy: 0.9534883720930233\n",
            "Batch 2 - Loss: 0.043375134494163754, Accuracy: 0.9767441860465116\n",
            "Batch 3 - Loss: 0.2319579046614012, Accuracy: 0.9354838709677419\n",
            "Batch 1 - Loss: 0.10987730824405947, Accuracy: 0.9534883720930233\n",
            "Batch 1 - Loss: 0.11419755931925922, Accuracy: 0.9534883720930233\n",
            "Batch 2 - Loss: 0.04745176566508731, Accuracy: 0.9767441860465116\n",
            "Batch 3 - Loss: 0.23206951336704243, Accuracy: 0.9354838709677419\n",
            "Batch 1 - Loss: 0.10775602685458402, Accuracy: 0.9534883720930233\n",
            "Batch 1 - Loss: 0.11515841774044268, Accuracy: 0.9534883720930233\n",
            "Batch 2 - Loss: 0.049644255084625735, Accuracy: 0.9767441860465116\n",
            "Batch 3 - Loss: 0.2302528718625727, Accuracy: 0.9354838709677419\n",
            "Batch 1 - Loss: 0.1065674691827777, Accuracy: 0.9534883720930233\n",
            "Batch 1 - Loss: 0.11533741536634783, Accuracy: 0.9534883720930233\n",
            "Batch 2 - Loss: 0.0508900197012689, Accuracy: 0.9767441860465116\n",
            "Batch 3 - Loss: 0.22994755980185658, Accuracy: 0.9354838709677419\n",
            "Batch 1 - Loss: 0.1059494485729981, Accuracy: 0.9534883720930233\n",
            "Batch 1 - Loss: 0.11531708411522403, Accuracy: 0.9534883720930233\n",
            "Batch 2 - Loss: 0.051742691292241516, Accuracy: 0.9767441860465116\n",
            "Batch 3 - Loss: 0.2294056555483476, Accuracy: 0.9354838709677419\n",
            "Batch 1 - Loss: 0.10551102356687589, Accuracy: 0.9534883720930233\n",
            "Batch 1 - Loss: 0.11523056315546615, Accuracy: 0.9534883720930233\n",
            "Batch 2 - Loss: 0.05232368791622663, Accuracy: 0.9767441860465116\n",
            "Batch 3 - Loss: 0.230702312610983, Accuracy: 0.9354838709677419\n",
            "Batch 1 - Loss: 0.10518833151769827, Accuracy: 0.9534883720930233\n",
            "Batch 1 - Loss: 0.1150513328309802, Accuracy: 0.9534883720930233\n",
            "Batch 2 - Loss: 0.05292607248305439, Accuracy: 0.9767441860465116\n",
            "Batch 3 - Loss: 0.22875726615619799, Accuracy: 0.9354838709677419\n",
            "Batch 1 - Loss: 0.10481007484074098, Accuracy: 0.9534883720930233\n",
            "Batch 1 - Loss: 0.11483418098542193, Accuracy: 0.9534883720930233\n",
            "Batch 2 - Loss: 0.05316678256682085, Accuracy: 0.9767441860465116\n",
            "Batch 3 - Loss: 0.22888188301492535, Accuracy: 0.9354838709677419\n",
            "Batch 1 - Loss: 0.10465749545211471, Accuracy: 0.9534883720930233\n",
            "Batch 1 - Loss: 0.11470072400526803, Accuracy: 0.9534883720930233\n",
            "Batch 2 - Loss: 0.05339989084613945, Accuracy: 0.9767441860465116\n",
            "Batch 3 - Loss: 0.22845956417653002, Accuracy: 0.9354838709677419\n",
            "Batch 1 - Loss: 0.11136165370935333, Accuracy: 0.9574468085106383\n",
            "Batch 1 - Loss: 0.0917906917973913, Accuracy: 0.9574468085106383\n",
            "Batch 2 - Loss: 0.07964614761388153, Accuracy: 0.9574468085106383\n",
            "Batch 3 - Loss: 0.38666674666972584, Accuracy: 0.8947368421052632\n",
            "Batch 1 - Loss: 0.27279124971543633, Accuracy: 0.8723404255319149\n",
            "Batch 1 - Loss: 0.03706904773128099, Accuracy: 0.9787234042553191\n",
            "Batch 2 - Loss: 0.04595450072178497, Accuracy: 0.9787234042553191\n",
            "Batch 3 - Loss: 0.5413088367442459, Accuracy: 0.8947368421052632\n",
            "Batch 1 - Loss: 0.1985415622173799, Accuracy: 0.9148936170212766\n",
            "Batch 1 - Loss: 0.12514312437372785, Accuracy: 0.9361702127659575\n",
            "Batch 2 - Loss: 0.1416181255997008, Accuracy: 0.9361702127659575\n",
            "Batch 3 - Loss: 0.3656894366812964, Accuracy: 0.8947368421052632\n",
            "Batch 1 - Loss: 0.19112259630719614, Accuracy: 0.9361702127659575\n",
            "Batch 1 - Loss: 0.10372695564288532, Accuracy: 0.9361702127659575\n",
            "Batch 2 - Loss: 0.11458768279736377, Accuracy: 0.9574468085106383\n",
            "Batch 3 - Loss: 0.38212965702643975, Accuracy: 0.8947368421052632\n",
            "Batch 1 - Loss: 0.19389966661871594, Accuracy: 0.9574468085106383\n",
            "Batch 1 - Loss: 0.09533668454895412, Accuracy: 0.9361702127659575\n",
            "Batch 2 - Loss: 0.10607495993637156, Accuracy: 0.9574468085106383\n",
            "Batch 3 - Loss: 0.3894662423283004, Accuracy: 0.8947368421052632\n",
            "Batch 1 - Loss: 0.19181004155848583, Accuracy: 0.9574468085106383\n",
            "Batch 1 - Loss: 0.09526264574151022, Accuracy: 0.9361702127659575\n",
            "Batch 2 - Loss: 0.10676219254989788, Accuracy: 0.9574468085106383\n",
            "Batch 3 - Loss: 0.38397463055878517, Accuracy: 0.8947368421052632\n",
            "Batch 1 - Loss: 0.18851232868508802, Accuracy: 0.9574468085106383\n",
            "Batch 1 - Loss: 0.09616201930190607, Accuracy: 0.9361702127659575\n",
            "Batch 2 - Loss: 0.10823898735114512, Accuracy: 0.9574468085106383\n",
            "Batch 3 - Loss: 0.3769662223406, Accuracy: 0.8947368421052632\n",
            "Batch 1 - Loss: 0.18580433179239664, Accuracy: 0.9574468085106383\n",
            "Batch 1 - Loss: 0.09656474716534123, Accuracy: 0.9361702127659575\n",
            "Batch 2 - Loss: 0.10902070160863075, Accuracy: 0.9574468085106383\n",
            "Batch 3 - Loss: 0.3715933018302456, Accuracy: 0.8947368421052632\n",
            "Batch 1 - Loss: 0.18369609561027134, Accuracy: 0.9574468085106383\n",
            "Batch 1 - Loss: 0.09677952376761885, Accuracy: 0.9361702127659575\n",
            "Batch 2 - Loss: 0.1095148550800531, Accuracy: 0.9574468085106383\n",
            "Batch 3 - Loss: 0.36740939153091884, Accuracy: 0.8947368421052632\n",
            "Batch 1 - Loss: 0.181966857599577, Accuracy: 0.9574468085106383\n",
            "Batch 1 - Loss: 0.09699578676852745, Accuracy: 0.9361702127659575\n",
            "Batch 2 - Loss: 0.10995527634085354, Accuracy: 0.9574468085106383\n",
            "Batch 3 - Loss: 0.3639121806474654, Accuracy: 0.8947368421052632\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_params"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EQMIpH5iI4UQ",
        "outputId": "326ca910-f328-41ab-c726-b45431869bbe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6, 86)"
            ]
          },
          "metadata": {},
          "execution_count": 221
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Validating using the best parameters and test"
      ],
      "metadata": {
        "id": "Czo0FkJHUOcA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_tries = 50  # Maximum number of times to try training the model\n",
        "tries = 0  # Counter for the number of tries\n",
        "\n",
        "while tries < max_tries:\n",
        "    # Training loop\n",
        "    for epoch in range(best_epochs):\n",
        "      for i in range(0, len(X_train), best_batch_size):\n",
        "        X_mini_batch = X_train[i:i+best_batch_size]\n",
        "        y_mini_batch = y_train[i:i+best_batch_size]\n",
        "\n",
        "        # One-hot encode y_true\n",
        "        oh_y_true_mini_batch = np.eye(3)[y_mini_batch]\n",
        "\n",
        "        # Forward pass\n",
        "        dense1.forward(X_mini_batch)\n",
        "        activation1.forward(dense1.z)\n",
        "        dense2.forward(activation1.activity)\n",
        "        activation2.forward(dense2.z)\n",
        "        dense_output.forward(activation2.activity)\n",
        "        probs = output_activation.forward(dense_output.z)\n",
        "\n",
        "        # Loss\n",
        "        loss = crossentropy.forward(probs, oh_y_true_mini_batch)\n",
        "\n",
        "        # Backward pass\n",
        "        dprobs = crossentropy.backward(probs, oh_y_true_mini_batch)\n",
        "        dactivation_output = output_activation.backward(dprobs)\n",
        "        dense_output.backward(dactivation_output)\n",
        "        dactivation2 = activation2.backward(dense_output.dinputs)\n",
        "        dense2.backward(dactivation2)\n",
        "        dactivation1 = activation1.backward(dense2.dinputs)\n",
        "        dense1.backward(dactivation1)\n",
        "\n",
        "        # Update weights and biases\n",
        "        optimizer.update_params(dense1, dense1.dweights, dense1.dbiases)\n",
        "        optimizer.update_params(dense2, dense2.dweights, dense2.dbiases)\n",
        "        optimizer.update_params(dense_output, dense_output.dweights,\n",
        "                                dense_output.dbiases)\n",
        "\n",
        "    # Validate the model\n",
        "    val_acc = validate_model(X_val, y_val)\n",
        "\n",
        "    # Check if the validation accuracy is above 90%\n",
        "    if val_acc >= 0.90:\n",
        "        # If so, evaluate the model on the test data\n",
        "        test_acc = validate_model(X_test, y_test)\n",
        "\n",
        "        # Check if the test accuracy is also above 90%\n",
        "        if test_acc >= 0.90:\n",
        "            print(f\"Model achieved {test_acc*100}% accuracy on the test data.\")\n",
        "            break\n",
        "        else:\n",
        "            print(f\"Model achieved {test_acc*100}% accuracy on the test data.\")\n",
        "    else:\n",
        "        print(f\"Model achieved {val_acc*100}% accuracy on the validation data.\")\n",
        "\n",
        "    tries += 1\n",
        "\n",
        "# Check if the model training was successful\n",
        "if tries == max_tries:\n",
        "    print(\"Failed to train a model with >= 90% accuracy after\", max_tries, \"tries.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zsQiU6FfFzD3",
        "outputId": "d5259504-6319-4b18-b547-91d483f769cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model achieved 93.5% accuracy on the test data.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print optimized weights and biases\n",
        "print(\"Optimized weights and biases after training:\")\n",
        "\n",
        "print(\"\\nDense Layer 1\")\n",
        "print(\"Weights:\")\n",
        "print(dense1.weights)\n",
        "print(\"Biases:\")\n",
        "print(dense1.biases)\n",
        "\n",
        "print(\"\\nDense Layer 2\")\n",
        "print(\"Weights:\")\n",
        "print(dense2.weights)\n",
        "print(\"Biases:\")\n",
        "print(dense2.biases)\n",
        "\n",
        "print(\"\\nOutput Dense Layer\")\n",
        "print(\"Weights:\")\n",
        "print(dense_output.weights)\n",
        "print(\"Biases:\")\n",
        "print(dense_output.biases)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nfUHQWqKDlX7",
        "outputId": "e3c67b36-21da-4c4a-a920-7e8c969c9525"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimized weights and biases after training:\n",
            "\n",
            "Dense Layer 1\n",
            "Weights:\n",
            "[[-0.49210911  2.68404544  2.27441565 -0.12224268]\n",
            " [-3.77885934  0.72159417  0.63082684 -0.87019258]\n",
            " [-0.348039    1.98207829  1.69554057 -0.08503729]]\n",
            "Biases:\n",
            "[[0.61600075 0.72308512 0.58225449 0.12767581]]\n",
            "\n",
            "Dense Layer 2\n",
            "Weights:\n",
            "[[-6.40038062e-03  9.26889222e-04 -1.50880383e-03  8.58907916e-01\n",
            "   2.79814423e-03 -1.75961605e-02 -1.81393882e-02  5.48709732e-01]\n",
            " [-2.07956641e-03  5.52002164e-03  2.85474080e-03  3.41173365e+00\n",
            "  -6.42012024e-03 -1.32684419e-03 -2.18276165e-04 -4.24879227e-01]\n",
            " [-3.79520256e-03 -3.62589104e-03 -1.71118452e-02  2.89264807e+00\n",
            "  -3.69463536e-03 -2.35854154e-03  3.80802025e-04 -3.98177417e-01]\n",
            " [-4.31167518e-03  1.17677777e-04 -1.98717132e-02  2.13488899e-01\n",
            "  -9.68103558e-03 -1.26518264e-02 -4.60992080e-03  1.02126971e-01]]\n",
            "Biases:\n",
            "[[ 0.         -0.01315308 -0.00020876 -0.00323128 -0.00900484  0.\n",
            "  -0.00052102  0.00047179]]\n",
            "\n",
            "Output Dense Layer\n",
            "Weights:\n",
            "[[-1.02928470e-03  1.31708524e-02 -1.07298570e-02]\n",
            " [ 1.44397033e-02 -5.15204102e-03  1.38242191e-04]\n",
            " [-1.18394434e-03 -1.73399209e-02 -1.87865733e-03]\n",
            " [ 1.11681474e+00  1.15391647e-01 -1.24624201e+00]\n",
            " [-5.36166520e-03 -2.52381391e-02  2.96300174e-04]\n",
            " [ 8.77828587e-03  9.91473327e-03 -3.62027222e-03]\n",
            " [-6.92084922e-04 -6.28018985e-05  7.01274765e-03]\n",
            " [-1.86839412e+00  1.24151469e+00  6.35100055e-01]]\n",
            "Biases:\n",
            "[[-4.42084728  0.6446339   3.77621338]]\n"
          ]
        }
      ]
    }
  ]
}
